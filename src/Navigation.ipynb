{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"../banana_unity_env/Banana.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.ac1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.ac2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def preprocess_input(self, state):\n",
    "        return state\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        # perform the forward pass\n",
    "        # x = self.preprocess_input(state)\n",
    "        x = self.ac1(self.fc1(state))\n",
    "        x = self.ac2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        # x = self.model(state)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.ac1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.ac2 = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "#         self.fc3 = nn.Linear(512, 64)\n",
    "#         self.ac3 = nn.ReLU()\n",
    "#         self.drop3 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def preprocess_input(self, state):\n",
    "        return state\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.drop1(self.ac1(self.fc1(state)))\n",
    "        x = self.drop2(self.ac2(self.fc2(x)))\n",
    "#         x = self.drop3(self.ac3(self.fc3(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "This part will present a vanilla version of the DQN algorithm, only with Experience Replay Buffer and Target Network. The next part will show the impact of adding the improvements presented in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 state_size, \n",
    "                 action_size,\n",
    "                 t_step=0,\n",
    "                 alpha=0.1,\n",
    "                 model_path=None,\n",
    "                 buffer_size=int(1e5),\n",
    "                 batch_size=64,\n",
    "                 gamma=0.99,\n",
    "                 tau=1e-3,\n",
    "                 learning_rate=5e-4,\n",
    "                 update_every=4,\n",
    "                 seed=0):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Q-Network\n",
    "        if not model_path:\n",
    "            print(f\"Creating new models for local and target networks.\")\n",
    "            self.qnetwork_local = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_target = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate)\n",
    "            self.training = True\n",
    "        else:\n",
    "            print(f\"Loading model from file {model_path}\")\n",
    "            self.qnetwork_local = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_target = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_local.load_state_dict(torch.load(model_path))\n",
    "            self.qnetwork_target.load_state_dict(torch.load(model_path))\n",
    "            self.training = False\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = PrioritizedReplayBuffer(action_size, buffer_size, batch_size, seed, self.device)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = t_step\n",
    "        self.alpha = alpha\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_every = update_every\n",
    "        \n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Compute target values: # reward + gamma * max(Q(s', a')) * (1-done)\n",
    "        td_targets_next_states = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        td_targets = rewards + (gamma*td_targets_next_states * (1-dones))\n",
    "#         print(f\"td_targets_next_states: {td_targets_next_states.shape}. Should be {rewards.shape[0]}\")\n",
    "        \n",
    "        # Current Q values\n",
    "        q_values_local = self.qnetwork_local(states).gather(1, actions) # Q\n",
    "        \n",
    "        # compute loss\n",
    "        loss = F.mse_loss(q_values_local, td_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- # # Only update the target network after we pass on the samples\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)                     \n",
    "\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "            \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "\n",
    "                \n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "            \n",
    "        if self.training:\n",
    "            self.qnetwork_local.train()\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer: # TODO: Implement Prioritized Experience Replay    \n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed, device):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training algorithm: Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def dqn(agent, env, brain_name, model_name=\"dqn_3fc_act_nodrop\", n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"\n",
    "    Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    best = 0\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]            \n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            env_info = env.step(action.astype(np.int32))[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]             \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=5.0 and np.mean(scores_window)>best:\n",
    "            best = np.mean(scores_window)\n",
    "            print(f\"\\nOverwriting last checkpiont with current average score: {best}\")\n",
    "            filename = model_name + timestr + f\"-score-{int(np.mean(scores_window))}\" + \".pth\"\n",
    "            torch.save(agent.qnetwork_local.state_dict(), filename)\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the environment and the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"../banana_unity_env/Banana.exe\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_step = 0\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.99\n",
    "agent = Agent(state_size=37, \n",
    "              action_size=4,\n",
    "              t_step=t_step,\n",
    "              alpha=alpha,\n",
    "              gamma=gamma,\n",
    "              seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Deep Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -0.01\n",
      "Episode 200\tAverage Score: 1.972\n",
      "Episode 300\tAverage Score: 6.67\n",
      "Episode 400\tAverage Score: 9.86\n",
      "Episode 500\tAverage Score: 11.77\n",
      "Episode 600\tAverage Score: 13.48\n",
      "Episode 700\tAverage Score: 14.02\n",
      "Episode 800\tAverage Score: 13.13\n",
      "Episode 900\tAverage Score: 13.41\n",
      "Episode 1000\tAverage Score: 12.42\n",
      "Episode 1100\tAverage Score: 14.12\n",
      "Episode 1200\tAverage Score: 14.01\n",
      "Episode 1300\tAverage Score: 13.05\n",
      "Episode 1400\tAverage Score: 12.89\n",
      "Episode 1442\tAverage Score: 12.93"
     ]
    }
   ],
   "source": [
    "# 4FC with act and dropout (max width=512)\n",
    "scores = dqn(agent, \n",
    "             env, \n",
    "             brain_name,\n",
    "             model_name=\"dqn_4f_act_drop\"\n",
    "             n_episodes=2000, \n",
    "             max_t=1000, \n",
    "             eps_start=1.0, \n",
    "             eps_end=0.01, \n",
    "             eps_decay=0.995)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.78\n",
      "Episode 200\tAverage Score: 4.27\n",
      "Episode 300\tAverage Score: 6.86\n",
      "Episode 400\tAverage Score: 10.49\n",
      "Episode 500\tAverage Score: 13.23\n",
      "Episode 600\tAverage Score: 13.57\n",
      "Episode 700\tAverage Score: 14.30\n",
      "Episode 739\tAverage Score: 15.00\n",
      "Environment solved in 639 episodes!\tAverage Score: 15.00\n",
      "Overwriting last checkpiont with current average score: 15.0\n",
      "Episode 740\tAverage Score: 15.09\n",
      "Environment solved in 640 episodes!\tAverage Score: 15.09\n",
      "Overwriting last checkpiont with current average score: 15.09\n",
      "Episode 742\tAverage Score: 15.11\n",
      "Environment solved in 642 episodes!\tAverage Score: 15.11\n",
      "Overwriting last checkpiont with current average score: 15.11\n",
      "Episode 743\tAverage Score: 15.16\n",
      "Environment solved in 643 episodes!\tAverage Score: 15.16\n",
      "Overwriting last checkpiont with current average score: 15.16\n",
      "Episode 745\tAverage Score: 15.19\n",
      "Environment solved in 645 episodes!\tAverage Score: 15.19\n",
      "Overwriting last checkpiont with current average score: 15.19\n",
      "Episode 746\tAverage Score: 15.26\n",
      "Environment solved in 646 episodes!\tAverage Score: 15.26\n",
      "Overwriting last checkpiont with current average score: 15.26\n",
      "Episode 790\tAverage Score: 15.32\n",
      "Environment solved in 690 episodes!\tAverage Score: 15.32\n",
      "Overwriting last checkpiont with current average score: 15.32\n",
      "Episode 791\tAverage Score: 15.33\n",
      "Environment solved in 691 episodes!\tAverage Score: 15.33\n",
      "Overwriting last checkpiont with current average score: 15.33\n",
      "Episode 792\tAverage Score: 15.34\n",
      "Environment solved in 692 episodes!\tAverage Score: 15.34\n",
      "Overwriting last checkpiont with current average score: 15.34\n",
      "Episode 800\tAverage Score: 14.86\n",
      "Episode 900\tAverage Score: 15.17\n",
      "Episode 921\tAverage Score: 15.35\n",
      "Environment solved in 821 episodes!\tAverage Score: 15.35\n",
      "Overwriting last checkpiont with current average score: 15.35\n",
      "Episode 1000\tAverage Score: 14.93\n",
      "Episode 1088\tAverage Score: 15.36\n",
      "Environment solved in 988 episodes!\tAverage Score: 15.36\n",
      "Overwriting last checkpiont with current average score: 15.36\n",
      "Episode 1089\tAverage Score: 15.41\n",
      "Environment solved in 989 episodes!\tAverage Score: 15.41\n",
      "Overwriting last checkpiont with current average score: 15.41\n",
      "Episode 1090\tAverage Score: 15.43\n",
      "Environment solved in 990 episodes!\tAverage Score: 15.43\n",
      "Overwriting last checkpiont with current average score: 15.43\n",
      "Episode 1091\tAverage Score: 15.44\n",
      "Environment solved in 991 episodes!\tAverage Score: 15.44\n",
      "Overwriting last checkpiont with current average score: 15.44\n",
      "Episode 1092\tAverage Score: 15.49\n",
      "Environment solved in 992 episodes!\tAverage Score: 15.49\n",
      "Overwriting last checkpiont with current average score: 15.49\n",
      "Episode 1095\tAverage Score: 15.65\n",
      "Environment solved in 995 episodes!\tAverage Score: 15.65\n",
      "Overwriting last checkpiont with current average score: 15.65\n",
      "Episode 1099\tAverage Score: 15.66\n",
      "Environment solved in 999 episodes!\tAverage Score: 15.66\n",
      "Overwriting last checkpiont with current average score: 15.66\n",
      "Episode 1100\tAverage Score: 15.61\n",
      "Episode 1103\tAverage Score: 15.70\n",
      "Environment solved in 1003 episodes!\tAverage Score: 15.70\n",
      "Overwriting last checkpiont with current average score: 15.7\n",
      "Episode 1104\tAverage Score: 15.72\n",
      "Environment solved in 1004 episodes!\tAverage Score: 15.72\n",
      "Overwriting last checkpiont with current average score: 15.72\n",
      "Episode 1105\tAverage Score: 15.78\n",
      "Environment solved in 1005 episodes!\tAverage Score: 15.78\n",
      "Overwriting last checkpiont with current average score: 15.78\n",
      "Episode 1118\tAverage Score: 15.81\n",
      "Environment solved in 1018 episodes!\tAverage Score: 15.81\n",
      "Overwriting last checkpiont with current average score: 15.81\n",
      "Episode 1141\tAverage Score: 15.88\n",
      "Environment solved in 1041 episodes!\tAverage Score: 15.88\n",
      "Overwriting last checkpiont with current average score: 15.88\n",
      "Episode 1142\tAverage Score: 15.95\n",
      "Environment solved in 1042 episodes!\tAverage Score: 15.95\n",
      "Overwriting last checkpiont with current average score: 15.95\n",
      "Episode 1143\tAverage Score: 15.98\n",
      "Environment solved in 1043 episodes!\tAverage Score: 15.98\n",
      "Overwriting last checkpiont with current average score: 15.98\n",
      "Episode 1144\tAverage Score: 15.99\n",
      "Environment solved in 1044 episodes!\tAverage Score: 15.99\n",
      "Overwriting last checkpiont with current average score: 15.99\n",
      "Episode 1148\tAverage Score: 16.11\n",
      "Environment solved in 1048 episodes!\tAverage Score: 16.11\n",
      "Overwriting last checkpiont with current average score: 16.11\n",
      "Episode 1149\tAverage Score: 16.16\n",
      "Environment solved in 1049 episodes!\tAverage Score: 16.16\n",
      "Overwriting last checkpiont with current average score: 16.16\n",
      "Episode 1150\tAverage Score: 16.18\n",
      "Environment solved in 1050 episodes!\tAverage Score: 16.18\n",
      "Overwriting last checkpiont with current average score: 16.18\n",
      "Episode 1160\tAverage Score: 16.24\n",
      "Environment solved in 1060 episodes!\tAverage Score: 16.24\n",
      "Overwriting last checkpiont with current average score: 16.24\n",
      "Episode 1161\tAverage Score: 16.33\n",
      "Environment solved in 1061 episodes!\tAverage Score: 16.33\n",
      "Overwriting last checkpiont with current average score: 16.33\n",
      "Episode 1162\tAverage Score: 16.39\n",
      "Environment solved in 1062 episodes!\tAverage Score: 16.39\n",
      "Overwriting last checkpiont with current average score: 16.39\n",
      "Episode 1163\tAverage Score: 16.44\n",
      "Environment solved in 1063 episodes!\tAverage Score: 16.44\n",
      "Overwriting last checkpiont with current average score: 16.44\n",
      "Episode 1166\tAverage Score: 16.45\n",
      "Environment solved in 1066 episodes!\tAverage Score: 16.45\n",
      "Overwriting last checkpiont with current average score: 16.45\n",
      "Episode 1171\tAverage Score: 16.52\n",
      "Environment solved in 1071 episodes!\tAverage Score: 16.52\n",
      "Overwriting last checkpiont with current average score: 16.52\n",
      "Episode 1172\tAverage Score: 16.56\n",
      "Environment solved in 1072 episodes!\tAverage Score: 16.56\n",
      "Overwriting last checkpiont with current average score: 16.56\n",
      "Episode 1175\tAverage Score: 16.66\n",
      "Environment solved in 1075 episodes!\tAverage Score: 16.66\n",
      "Overwriting last checkpiont with current average score: 16.66\n",
      "Episode 1200\tAverage Score: 16.50\n",
      "Episode 1223\tAverage Score: 16.69\n",
      "Environment solved in 1123 episodes!\tAverage Score: 16.69\n",
      "Overwriting last checkpiont with current average score: 16.69\n",
      "Episode 1224\tAverage Score: 16.71\n",
      "Environment solved in 1124 episodes!\tAverage Score: 16.71\n",
      "Overwriting last checkpiont with current average score: 16.71\n",
      "Episode 1225\tAverage Score: 16.78\n",
      "Environment solved in 1125 episodes!\tAverage Score: 16.78\n",
      "Overwriting last checkpiont with current average score: 16.78\n",
      "Episode 1226\tAverage Score: 16.92\n",
      "Environment solved in 1126 episodes!\tAverage Score: 16.92\n",
      "Overwriting last checkpiont with current average score: 16.92\n",
      "Episode 1227\tAverage Score: 16.98\n",
      "Environment solved in 1127 episodes!\tAverage Score: 16.98\n",
      "Overwriting last checkpiont with current average score: 16.98\n",
      "Episode 1300\tAverage Score: 15.63\n",
      "Episode 1400\tAverage Score: 17.03\n",
      "\n",
      "Environment solved in 1300 episodes!\tAverage Score: 17.03\n",
      "Overwriting last checkpiont with current average score: 17.03\n",
      "Episode 1420\tAverage Score: 17.16\n",
      "Environment solved in 1320 episodes!\tAverage Score: 17.16\n",
      "Overwriting last checkpiont with current average score: 17.16\n",
      "Episode 1500\tAverage Score: 16.31\n",
      "Episode 1600\tAverage Score: 15.59\n",
      "Episode 1700\tAverage Score: 16.10\n",
      "Episode 1800\tAverage Score: 14.89\n",
      "Episode 1900\tAverage Score: 15.95\n",
      "Episode 2000\tAverage Score: 15.55\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3 FC with act (max=64)\n",
    "scores = dqn(agent, \n",
    "             env, \n",
    "             brain_name, \n",
    "             n_episodes=2000, \n",
    "             max_t=1000, \n",
    "             eps_start=1.0, \n",
    "             eps_end=0.01, \n",
    "             eps_decay=0.995)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.78\n",
      "Episode 200\tAverage Score: 4.27\n",
      "Episode 228\tAverage Score: 5.01\n",
      "Overwriting last checkpiont with current average score: 5.01\n",
      "Episode 232\tAverage Score: 5.05\n",
      "Overwriting last checkpiont with current average score: 5.05\n",
      "Episode 236\tAverage Score: 5.07\n",
      "Overwriting last checkpiont with current average score: 5.07\n",
      "Episode 239\tAverage Score: 5.10\n",
      "Overwriting last checkpiont with current average score: 5.1\n",
      "Episode 242\tAverage Score: 5.16\n",
      "Overwriting last checkpiont with current average score: 5.16\n",
      "Episode 243\tAverage Score: 5.24\n",
      "Overwriting last checkpiont with current average score: 5.24\n",
      "Episode 244\tAverage Score: 5.34\n",
      "Overwriting last checkpiont with current average score: 5.34\n",
      "Episode 245\tAverage Score: 5.41\n",
      "Overwriting last checkpiont with current average score: 5.41\n",
      "Episode 248\tAverage Score: 5.44\n",
      "Overwriting last checkpiont with current average score: 5.44\n",
      "Episode 249\tAverage Score: 5.45\n",
      "Overwriting last checkpiont with current average score: 5.45\n",
      "Episode 250\tAverage Score: 5.54\n",
      "Overwriting last checkpiont with current average score: 5.54\n",
      "Episode 252\tAverage Score: 5.57\n",
      "Overwriting last checkpiont with current average score: 5.57\n",
      "Episode 253\tAverage Score: 5.60\n",
      "Overwriting last checkpiont with current average score: 5.6\n",
      "Episode 254\tAverage Score: 5.61\n",
      "Overwriting last checkpiont with current average score: 5.61\n",
      "Episode 255\tAverage Score: 5.69\n",
      "Overwriting last checkpiont with current average score: 5.69\n",
      "Episode 256\tAverage Score: 5.70\n",
      "Overwriting last checkpiont with current average score: 5.7\n",
      "Episode 257\tAverage Score: 5.71\n",
      "Overwriting last checkpiont with current average score: 5.71\n",
      "Episode 258\tAverage Score: 5.77\n",
      "Overwriting last checkpiont with current average score: 5.77\n",
      "Episode 275\tAverage Score: 5.85\n",
      "Overwriting last checkpiont with current average score: 5.85\n",
      "Episode 276\tAverage Score: 5.89\n",
      "Overwriting last checkpiont with current average score: 5.89\n",
      "Episode 277\tAverage Score: 5.97\n",
      "Overwriting last checkpiont with current average score: 5.97\n",
      "Episode 278\tAverage Score: 5.98\n",
      "Overwriting last checkpiont with current average score: 5.98\n",
      "Episode 280\tAverage Score: 6.00\n",
      "Overwriting last checkpiont with current average score: 6.0\n",
      "Episode 281\tAverage Score: 6.04\n",
      "Overwriting last checkpiont with current average score: 6.04\n",
      "Episode 282\tAverage Score: 6.05\n",
      "Overwriting last checkpiont with current average score: 6.05\n",
      "Episode 283\tAverage Score: 6.09\n",
      "Overwriting last checkpiont with current average score: 6.09\n",
      "Episode 284\tAverage Score: 6.19\n",
      "Overwriting last checkpiont with current average score: 6.19\n",
      "Episode 285\tAverage Score: 6.26\n",
      "Overwriting last checkpiont with current average score: 6.26\n",
      "Episode 286\tAverage Score: 6.28\n",
      "Overwriting last checkpiont with current average score: 6.28\n",
      "Episode 287\tAverage Score: 6.37\n",
      "Overwriting last checkpiont with current average score: 6.37\n",
      "Episode 288\tAverage Score: 6.41\n",
      "Overwriting last checkpiont with current average score: 6.41\n",
      "Episode 289\tAverage Score: 6.47\n",
      "Overwriting last checkpiont with current average score: 6.47\n",
      "Episode 290\tAverage Score: 6.48\n",
      "Overwriting last checkpiont with current average score: 6.48\n",
      "Episode 291\tAverage Score: 6.50\n",
      "Overwriting last checkpiont with current average score: 6.5\n",
      "Episode 292\tAverage Score: 6.56\n",
      "Overwriting last checkpiont with current average score: 6.56\n",
      "Episode 295\tAverage Score: 6.60\n",
      "Overwriting last checkpiont with current average score: 6.6\n",
      "Episode 296\tAverage Score: 6.62\n",
      "Overwriting last checkpiont with current average score: 6.62\n",
      "Episode 297\tAverage Score: 6.66\n",
      "Overwriting last checkpiont with current average score: 6.66\n",
      "Episode 298\tAverage Score: 6.73\n",
      "Overwriting last checkpiont with current average score: 6.73\n",
      "Episode 299\tAverage Score: 6.79\n",
      "Overwriting last checkpiont with current average score: 6.79\n",
      "Episode 300\tAverage Score: 6.86\n",
      "\n",
      "Overwriting last checkpiont with current average score: 6.86\n",
      "Episode 302\tAverage Score: 6.96\n",
      "Overwriting last checkpiont with current average score: 6.96\n",
      "Episode 303\tAverage Score: 7.05\n",
      "Overwriting last checkpiont with current average score: 7.05\n",
      "Episode 304\tAverage Score: 7.11\n",
      "Overwriting last checkpiont with current average score: 7.11\n",
      "Episode 305\tAverage Score: 7.16\n",
      "Overwriting last checkpiont with current average score: 7.16\n",
      "Episode 306\tAverage Score: 7.19\n",
      "Overwriting last checkpiont with current average score: 7.19\n",
      "Episode 307\tAverage Score: 7.27\n",
      "Overwriting last checkpiont with current average score: 7.27\n",
      "Episode 308\tAverage Score: 7.28\n",
      "Overwriting last checkpiont with current average score: 7.28\n",
      "Episode 311\tAverage Score: 7.32\n",
      "Overwriting last checkpiont with current average score: 7.32\n",
      "Episode 313\tAverage Score: 7.38\n",
      "Overwriting last checkpiont with current average score: 7.38\n",
      "Episode 314\tAverage Score: 7.41\n",
      "Overwriting last checkpiont with current average score: 7.41\n",
      "Episode 316\tAverage Score: 7.42\n",
      "Overwriting last checkpiont with current average score: 7.42\n",
      "Episode 319\tAverage Score: 7.43\n",
      "Overwriting last checkpiont with current average score: 7.43\n",
      "Episode 320\tAverage Score: 7.55\n",
      "Overwriting last checkpiont with current average score: 7.55\n",
      "Episode 322\tAverage Score: 7.58\n",
      "Overwriting last checkpiont with current average score: 7.58\n",
      "Episode 323\tAverage Score: 7.61\n",
      "Overwriting last checkpiont with current average score: 7.61\n",
      "Episode 324\tAverage Score: 7.63\n",
      "Overwriting last checkpiont with current average score: 7.63\n",
      "Episode 331\tAverage Score: 7.65\n",
      "Overwriting last checkpiont with current average score: 7.65\n",
      "Episode 333\tAverage Score: 7.68\n",
      "Overwriting last checkpiont with current average score: 7.68\n",
      "Episode 334\tAverage Score: 7.75\n",
      "Overwriting last checkpiont with current average score: 7.75\n",
      "Episode 335\tAverage Score: 7.80\n",
      "Overwriting last checkpiont with current average score: 7.8\n",
      "Episode 336\tAverage Score: 7.94\n",
      "Overwriting last checkpiont with current average score: 7.94\n",
      "Episode 337\tAverage Score: 7.96\n",
      "Overwriting last checkpiont with current average score: 7.96\n",
      "Episode 338\tAverage Score: 8.06\n",
      "Overwriting last checkpiont with current average score: 8.06\n",
      "Episode 341\tAverage Score: 8.10\n",
      "Overwriting last checkpiont with current average score: 8.1\n",
      "Episode 350\tAverage Score: 8.14\n",
      "Overwriting last checkpiont with current average score: 8.14\n",
      "Episode 351\tAverage Score: 8.21\n",
      "Overwriting last checkpiont with current average score: 8.21\n",
      "Episode 353\tAverage Score: 8.22\n",
      "Overwriting last checkpiont with current average score: 8.22\n",
      "Episode 354\tAverage Score: 8.27\n",
      "Overwriting last checkpiont with current average score: 8.27\n",
      "Episode 355\tAverage Score: 8.34\n",
      "Overwriting last checkpiont with current average score: 8.34\n",
      "Episode 356\tAverage Score: 8.42\n",
      "Overwriting last checkpiont with current average score: 8.42\n",
      "Episode 357\tAverage Score: 8.52\n",
      "Overwriting last checkpiont with current average score: 8.52\n",
      "Episode 358\tAverage Score: 8.54\n",
      "Overwriting last checkpiont with current average score: 8.54\n",
      "Episode 359\tAverage Score: 8.66\n",
      "Overwriting last checkpiont with current average score: 8.66\n",
      "Episode 360\tAverage Score: 8.68\n",
      "Overwriting last checkpiont with current average score: 8.68\n",
      "Episode 361\tAverage Score: 8.69\n",
      "Overwriting last checkpiont with current average score: 8.69\n",
      "Episode 362\tAverage Score: 8.80\n",
      "Overwriting last checkpiont with current average score: 8.8\n",
      "Episode 363\tAverage Score: 8.81\n",
      "Overwriting last checkpiont with current average score: 8.81\n",
      "Episode 364\tAverage Score: 8.86\n",
      "Overwriting last checkpiont with current average score: 8.86\n",
      "Episode 365\tAverage Score: 8.97\n",
      "Overwriting last checkpiont with current average score: 8.97\n",
      "Episode 366\tAverage Score: 9.04\n",
      "Overwriting last checkpiont with current average score: 9.04\n",
      "Episode 368\tAverage Score: 9.05\n",
      "Overwriting last checkpiont with current average score: 9.05\n",
      "Episode 369\tAverage Score: 9.16\n",
      "Overwriting last checkpiont with current average score: 9.16\n",
      "Episode 370\tAverage Score: 9.17\n",
      "Overwriting last checkpiont with current average score: 9.17\n",
      "Episode 371\tAverage Score: 9.29\n",
      "Overwriting last checkpiont with current average score: 9.29\n",
      "Episode 372\tAverage Score: 9.34\n",
      "Overwriting last checkpiont with current average score: 9.34\n",
      "Episode 373\tAverage Score: 9.42\n",
      "Overwriting last checkpiont with current average score: 9.42\n",
      "Episode 375\tAverage Score: 9.44\n",
      "Overwriting last checkpiont with current average score: 9.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 378\tAverage Score: 9.51\n",
      "Overwriting last checkpiont with current average score: 9.51\n",
      "Episode 379\tAverage Score: 9.59\n",
      "Overwriting last checkpiont with current average score: 9.59\n",
      "Episode 381\tAverage Score: 9.62\n",
      "Overwriting last checkpiont with current average score: 9.62\n",
      "Episode 383\tAverage Score: 9.69\n",
      "Overwriting last checkpiont with current average score: 9.69\n",
      "Episode 384\tAverage Score: 9.77\n",
      "Overwriting last checkpiont with current average score: 9.77\n",
      "Episode 385\tAverage Score: 9.80\n",
      "Overwriting last checkpiont with current average score: 9.8\n",
      "Episode 386\tAverage Score: 9.87\n",
      "Overwriting last checkpiont with current average score: 9.87\n",
      "Episode 388\tAverage Score: 9.94\n",
      "Overwriting last checkpiont with current average score: 9.94\n",
      "Episode 389\tAverage Score: 9.99\n",
      "Overwriting last checkpiont with current average score: 9.99\n",
      "Episode 390\tAverage Score: 10.04\n",
      "Overwriting last checkpiont with current average score: 10.04\n",
      "Episode 391\tAverage Score: 10.10\n",
      "Overwriting last checkpiont with current average score: 10.1\n",
      "Episode 392\tAverage Score: 10.14\n",
      "Overwriting last checkpiont with current average score: 10.14\n",
      "Episode 393\tAverage Score: 10.29\n",
      "Overwriting last checkpiont with current average score: 10.29\n",
      "Episode 394\tAverage Score: 10.32\n",
      "Overwriting last checkpiont with current average score: 10.32\n",
      "Episode 395\tAverage Score: 10.34\n",
      "Overwriting last checkpiont with current average score: 10.34\n",
      "Episode 396\tAverage Score: 10.37\n",
      "Overwriting last checkpiont with current average score: 10.37\n",
      "Episode 397\tAverage Score: 10.39\n",
      "Overwriting last checkpiont with current average score: 10.39\n",
      "Episode 398\tAverage Score: 10.42\n",
      "Overwriting last checkpiont with current average score: 10.42\n",
      "Episode 399\tAverage Score: 10.47\n",
      "Overwriting last checkpiont with current average score: 10.47\n",
      "Episode 400\tAverage Score: 10.49\n",
      "\n",
      "Overwriting last checkpiont with current average score: 10.49\n",
      "Episode 401\tAverage Score: 10.55\n",
      "Overwriting last checkpiont with current average score: 10.55\n",
      "Episode 402\tAverage Score: 10.59\n",
      "Overwriting last checkpiont with current average score: 10.59\n",
      "Episode 403\tAverage Score: 10.60\n",
      "Overwriting last checkpiont with current average score: 10.6\n",
      "Episode 404\tAverage Score: 10.67\n",
      "Overwriting last checkpiont with current average score: 10.67\n",
      "Episode 412\tAverage Score: 10.81\n",
      "Overwriting last checkpiont with current average score: 10.81\n",
      "Episode 413\tAverage Score: 10.88\n",
      "Overwriting last checkpiont with current average score: 10.88\n",
      "Episode 414\tAverage Score: 10.90\n",
      "Overwriting last checkpiont with current average score: 10.9\n",
      "Episode 415\tAverage Score: 10.96\n",
      "Overwriting last checkpiont with current average score: 10.96\n",
      "Episode 416\tAverage Score: 11.08\n",
      "Overwriting last checkpiont with current average score: 11.08\n",
      "Episode 417\tAverage Score: 11.16\n",
      "Overwriting last checkpiont with current average score: 11.16\n",
      "Episode 423\tAverage Score: 11.17\n",
      "Overwriting last checkpiont with current average score: 11.17\n",
      "Episode 424\tAverage Score: 11.20\n",
      "Overwriting last checkpiont with current average score: 11.2\n",
      "Episode 425\tAverage Score: 11.26\n",
      "Overwriting last checkpiont with current average score: 11.26\n",
      "Episode 426\tAverage Score: 11.37\n",
      "Overwriting last checkpiont with current average score: 11.37\n",
      "Episode 427\tAverage Score: 11.48\n",
      "Overwriting last checkpiont with current average score: 11.48\n",
      "Episode 428\tAverage Score: 11.59\n",
      "Overwriting last checkpiont with current average score: 11.59\n",
      "Episode 429\tAverage Score: 11.68\n",
      "Overwriting last checkpiont with current average score: 11.68\n",
      "Episode 430\tAverage Score: 11.70\n",
      "Overwriting last checkpiont with current average score: 11.7\n",
      "Episode 431\tAverage Score: 11.76\n",
      "Overwriting last checkpiont with current average score: 11.76\n",
      "Episode 432\tAverage Score: 11.78\n",
      "Overwriting last checkpiont with current average score: 11.78\n",
      "Episode 433\tAverage Score: 11.85\n",
      "Overwriting last checkpiont with current average score: 11.85\n",
      "Episode 435\tAverage Score: 11.91\n",
      "Overwriting last checkpiont with current average score: 11.91\n",
      "Episode 438\tAverage Score: 11.92\n",
      "Overwriting last checkpiont with current average score: 11.92\n",
      "Episode 439\tAverage Score: 12.04\n",
      "Overwriting last checkpiont with current average score: 12.04\n",
      "Episode 440\tAverage Score: 12.11\n",
      "Overwriting last checkpiont with current average score: 12.11\n",
      "Episode 441\tAverage Score: 12.14\n",
      "Overwriting last checkpiont with current average score: 12.14\n",
      "Episode 442\tAverage Score: 12.20\n",
      "Overwriting last checkpiont with current average score: 12.2\n",
      "Episode 443\tAverage Score: 12.25\n",
      "Overwriting last checkpiont with current average score: 12.25\n",
      "Episode 445\tAverage Score: 12.27\n",
      "Overwriting last checkpiont with current average score: 12.27\n",
      "Episode 446\tAverage Score: 12.45\n",
      "Overwriting last checkpiont with current average score: 12.45\n",
      "Episode 447\tAverage Score: 12.54\n",
      "Overwriting last checkpiont with current average score: 12.54\n",
      "Episode 448\tAverage Score: 12.57\n",
      "Overwriting last checkpiont with current average score: 12.57\n",
      "Episode 449\tAverage Score: 12.68\n",
      "Overwriting last checkpiont with current average score: 12.68\n",
      "Episode 463\tAverage Score: 12.70\n",
      "Overwriting last checkpiont with current average score: 12.7\n",
      "Episode 464\tAverage Score: 12.77\n",
      "Overwriting last checkpiont with current average score: 12.77\n",
      "Episode 465\tAverage Score: 12.78\n",
      "Overwriting last checkpiont with current average score: 12.78\n",
      "Episode 466\tAverage Score: 12.87\n",
      "Overwriting last checkpiont with current average score: 12.87\n",
      "Episode 467\tAverage Score: 12.91\n",
      "Overwriting last checkpiont with current average score: 12.91\n",
      "Episode 468\tAverage Score: 12.95\n",
      "Overwriting last checkpiont with current average score: 12.95\n",
      "Episode 470\tAverage Score: 12.97\n",
      "Overwriting last checkpiont with current average score: 12.97\n",
      "Episode 474\tAverage Score: 13.01\n",
      "Overwriting last checkpiont with current average score: 13.01\n",
      "Episode 475\tAverage Score: 13.04\n",
      "Overwriting last checkpiont with current average score: 13.04\n",
      "Episode 476\tAverage Score: 13.15\n",
      "Overwriting last checkpiont with current average score: 13.15\n",
      "Episode 478\tAverage Score: 13.19\n",
      "Overwriting last checkpiont with current average score: 13.19\n",
      "Episode 479\tAverage Score: 13.22\n",
      "Overwriting last checkpiont with current average score: 13.22\n",
      "Episode 480\tAverage Score: 13.29\n",
      "Overwriting last checkpiont with current average score: 13.29\n",
      "Episode 481\tAverage Score: 13.31\n",
      "Overwriting last checkpiont with current average score: 13.31\n",
      "Episode 500\tAverage Score: 13.23\n",
      "Episode 505\tAverage Score: 13.34\n",
      "Overwriting last checkpiont with current average score: 13.34\n",
      "Episode 507\tAverage Score: 13.38\n",
      "Overwriting last checkpiont with current average score: 13.38\n",
      "Episode 508\tAverage Score: 13.48\n",
      "Overwriting last checkpiont with current average score: 13.48\n",
      "Episode 522\tAverage Score: 13.54\n",
      "Overwriting last checkpiont with current average score: 13.54\n",
      "Episode 525\tAverage Score: 13.55\n",
      "Overwriting last checkpiont with current average score: 13.55\n",
      "Episode 537\tAverage Score: 13.57\n",
      "Overwriting last checkpiont with current average score: 13.57\n",
      "Episode 560\tAverage Score: 13.58\n",
      "Overwriting last checkpiont with current average score: 13.58\n",
      "Episode 561\tAverage Score: 13.70\n",
      "Overwriting last checkpiont with current average score: 13.7\n",
      "Episode 600\tAverage Score: 13.57\n",
      "Episode 603\tAverage Score: 13.71\n",
      "Overwriting last checkpiont with current average score: 13.71\n",
      "Episode 606\tAverage Score: 13.76\n",
      "Overwriting last checkpiont with current average score: 13.76\n",
      "Episode 610\tAverage Score: 13.87\n",
      "Overwriting last checkpiont with current average score: 13.87\n",
      "Episode 612\tAverage Score: 13.90\n",
      "Overwriting last checkpiont with current average score: 13.9\n",
      "Episode 617\tAverage Score: 13.95\n",
      "Overwriting last checkpiont with current average score: 13.95\n",
      "Episode 627\tAverage Score: 13.96\n",
      "Overwriting last checkpiont with current average score: 13.96\n",
      "Episode 675\tAverage Score: 14.00\n",
      "Overwriting last checkpiont with current average score: 14.0\n",
      "Episode 676\tAverage Score: 14.01\n",
      "Overwriting last checkpiont with current average score: 14.01\n",
      "Episode 678\tAverage Score: 14.02\n",
      "Overwriting last checkpiont with current average score: 14.02\n",
      "Episode 681\tAverage Score: 14.03\n",
      "Overwriting last checkpiont with current average score: 14.03\n",
      "Episode 682\tAverage Score: 14.14\n",
      "Overwriting last checkpiont with current average score: 14.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 694\tAverage Score: 14.16\n",
      "Overwriting last checkpiont with current average score: 14.16\n",
      "Episode 695\tAverage Score: 14.18\n",
      "Overwriting last checkpiont with current average score: 14.18\n",
      "Episode 696\tAverage Score: 14.19\n",
      "Overwriting last checkpiont with current average score: 14.19\n",
      "Episode 697\tAverage Score: 14.24\n",
      "Overwriting last checkpiont with current average score: 14.24\n",
      "Episode 698\tAverage Score: 14.25\n",
      "Overwriting last checkpiont with current average score: 14.25\n",
      "Episode 699\tAverage Score: 14.28\n",
      "Overwriting last checkpiont with current average score: 14.28\n",
      "Episode 700\tAverage Score: 14.30\n",
      "\n",
      "Overwriting last checkpiont with current average score: 14.3\n",
      "Episode 713\tAverage Score: 14.32\n",
      "Overwriting last checkpiont with current average score: 14.32\n",
      "Episode 716\tAverage Score: 14.41\n",
      "Overwriting last checkpiont with current average score: 14.41\n",
      "Episode 719\tAverage Score: 14.44\n",
      "Overwriting last checkpiont with current average score: 14.44\n",
      "Episode 720\tAverage Score: 14.51\n",
      "Overwriting last checkpiont with current average score: 14.51\n",
      "Episode 721\tAverage Score: 14.57\n",
      "Overwriting last checkpiont with current average score: 14.57\n",
      "Episode 722\tAverage Score: 14.58\n",
      "Overwriting last checkpiont with current average score: 14.58\n",
      "Episode 731\tAverage Score: 14.74\n",
      "Overwriting last checkpiont with current average score: 14.74\n",
      "Episode 735\tAverage Score: 14.75\n",
      "Overwriting last checkpiont with current average score: 14.75\n",
      "Episode 736\tAverage Score: 14.87\n",
      "Overwriting last checkpiont with current average score: 14.87\n",
      "Episode 737\tAverage Score: 14.94\n",
      "Overwriting last checkpiont with current average score: 14.94\n",
      "Episode 738\tAverage Score: 14.99\n",
      "Overwriting last checkpiont with current average score: 14.99\n",
      "Episode 739\tAverage Score: 15.00\n",
      "Overwriting last checkpiont with current average score: 15.0\n",
      "Episode 740\tAverage Score: 15.09\n",
      "Overwriting last checkpiont with current average score: 15.09\n",
      "Episode 742\tAverage Score: 15.11\n",
      "Overwriting last checkpiont with current average score: 15.11\n",
      "Episode 743\tAverage Score: 15.16\n",
      "Overwriting last checkpiont with current average score: 15.16\n",
      "Episode 745\tAverage Score: 15.19\n",
      "Overwriting last checkpiont with current average score: 15.19\n",
      "Episode 746\tAverage Score: 15.26\n",
      "Overwriting last checkpiont with current average score: 15.26\n",
      "Episode 790\tAverage Score: 15.32\n",
      "Overwriting last checkpiont with current average score: 15.32\n",
      "Episode 791\tAverage Score: 15.33\n",
      "Overwriting last checkpiont with current average score: 15.33\n",
      "Episode 792\tAverage Score: 15.34\n",
      "Overwriting last checkpiont with current average score: 15.34\n",
      "Episode 800\tAverage Score: 14.86\n",
      "Episode 900\tAverage Score: 15.17\n",
      "Episode 921\tAverage Score: 15.35\n",
      "Overwriting last checkpiont with current average score: 15.35\n",
      "Episode 1000\tAverage Score: 14.93\n",
      "Episode 1088\tAverage Score: 15.36\n",
      "Overwriting last checkpiont with current average score: 15.36\n",
      "Episode 1089\tAverage Score: 15.41\n",
      "Overwriting last checkpiont with current average score: 15.41\n",
      "Episode 1090\tAverage Score: 15.43\n",
      "Overwriting last checkpiont with current average score: 15.43\n",
      "Episode 1091\tAverage Score: 15.44\n",
      "Overwriting last checkpiont with current average score: 15.44\n",
      "Episode 1092\tAverage Score: 15.49\n",
      "Overwriting last checkpiont with current average score: 15.49\n",
      "Episode 1095\tAverage Score: 15.65\n",
      "Overwriting last checkpiont with current average score: 15.65\n",
      "Episode 1099\tAverage Score: 15.66\n",
      "Overwriting last checkpiont with current average score: 15.66\n",
      "Episode 1100\tAverage Score: 15.61\n",
      "Episode 1103\tAverage Score: 15.70\n",
      "Overwriting last checkpiont with current average score: 15.7\n",
      "Episode 1104\tAverage Score: 15.72\n",
      "Overwriting last checkpiont with current average score: 15.72\n",
      "Episode 1105\tAverage Score: 15.78\n",
      "Overwriting last checkpiont with current average score: 15.78\n",
      "Episode 1118\tAverage Score: 15.81\n",
      "Overwriting last checkpiont with current average score: 15.81\n",
      "Episode 1141\tAverage Score: 15.88\n",
      "Overwriting last checkpiont with current average score: 15.88\n",
      "Episode 1142\tAverage Score: 15.95\n",
      "Overwriting last checkpiont with current average score: 15.95\n",
      "Episode 1143\tAverage Score: 15.98\n",
      "Overwriting last checkpiont with current average score: 15.98\n",
      "Episode 1144\tAverage Score: 15.99\n",
      "Overwriting last checkpiont with current average score: 15.99\n",
      "Episode 1148\tAverage Score: 16.11\n",
      "Overwriting last checkpiont with current average score: 16.11\n",
      "Episode 1149\tAverage Score: 16.16\n",
      "Overwriting last checkpiont with current average score: 16.16\n",
      "Episode 1150\tAverage Score: 16.18\n",
      "Overwriting last checkpiont with current average score: 16.18\n",
      "Episode 1160\tAverage Score: 16.24\n",
      "Overwriting last checkpiont with current average score: 16.24\n",
      "Episode 1161\tAverage Score: 16.33\n",
      "Overwriting last checkpiont with current average score: 16.33\n",
      "Episode 1162\tAverage Score: 16.39\n",
      "Overwriting last checkpiont with current average score: 16.39\n",
      "Episode 1163\tAverage Score: 16.44\n",
      "Overwriting last checkpiont with current average score: 16.44\n",
      "Episode 1166\tAverage Score: 16.45\n",
      "Overwriting last checkpiont with current average score: 16.45\n",
      "Episode 1171\tAverage Score: 16.52\n",
      "Overwriting last checkpiont with current average score: 16.52\n",
      "Episode 1172\tAverage Score: 16.56\n",
      "Overwriting last checkpiont with current average score: 16.56\n",
      "Episode 1175\tAverage Score: 16.66\n",
      "Overwriting last checkpiont with current average score: 16.66\n",
      "Episode 1200\tAverage Score: 16.50\n",
      "Episode 1223\tAverage Score: 16.69\n",
      "Overwriting last checkpiont with current average score: 16.69\n",
      "Episode 1224\tAverage Score: 16.71\n",
      "Overwriting last checkpiont with current average score: 16.71\n",
      "Episode 1225\tAverage Score: 16.78\n",
      "Overwriting last checkpiont with current average score: 16.78\n",
      "Episode 1226\tAverage Score: 16.92\n",
      "Overwriting last checkpiont with current average score: 16.92\n",
      "Episode 1227\tAverage Score: 16.98\n",
      "Overwriting last checkpiont with current average score: 16.98\n",
      "Episode 1300\tAverage Score: 15.63\n",
      "Episode 1400\tAverage Score: 17.03\n",
      "\n",
      "Overwriting last checkpiont with current average score: 17.03\n",
      "Episode 1420\tAverage Score: 17.16\n",
      "Overwriting last checkpiont with current average score: 17.16\n",
      "Episode 1500\tAverage Score: 16.31\n",
      "Episode 1600\tAverage Score: 15.59\n",
      "Episode 1655\tAverage Score: 16.06"
     ]
    }
   ],
   "source": [
    "# 3 FC with act (max width = 64)\n",
    "scores = dqn(agent, \n",
    "             env, \n",
    "             brain_name,\n",
    "             model_name=\"dqn_3fc_act_nodrop\",\n",
    "             n_episodes=2000, \n",
    "             max_t=1000, \n",
    "             eps_start=1.0, \n",
    "             eps_end=0.01, \n",
    "             eps_decay=0.995)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def test_agent(agent, env, brain_name, times=5, max_t=500, freq=60):\n",
    "    scores = []\n",
    "    for i in range(times):\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        state = env_info.vector_observations[0]         \n",
    "        score = 0                                       \n",
    "        for i in range(max_t):\n",
    "            action = agent.act(state, eps=0) # eps = 0 means the policy is 100% greedy\n",
    "            env_info = env.step(action.astype(np.int32))[brain_name] \n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]                \n",
    "            done = env_info.local_done[0]\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "            time.sleep(1/freq)\n",
    "        \n",
    "        scores.append(score)\n",
    "        print(f\"Episode ended with score: {score}. Starting a new one\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from file ./dqn_3fc_act_nodrop_avg17.pth\n",
      "Episode ended with score: 16.0. Starting a new one\n",
      "Episode ended with score: 11.0. Starting a new one\n",
      "Episode ended with score: 14.0. Starting a new one\n",
      "Episode ended with score: 19.0. Starting a new one\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./dqn_3fc_act_nodrop_avg17.pth\"\n",
    "\n",
    "t_step = 0\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.99\n",
    "agent = Agent(state_size=37, \n",
    "              action_size=4,\n",
    "              t_step=t_step,\n",
    "              alpha=alpha,\n",
    "              model_path=model_path,\n",
    "              seed=0)\n",
    "\n",
    "test_agent(agent, env, brain_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing improvements: \n",
    "\n",
    "### Double DQN, Dueling DQN and Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from model import QNetwork\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \"\"\"\n",
    "        TODO: IMPROVE MODEL\n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.ac1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.ac2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: IMPLEMENT DUELING DQNS: SEPARATE VALUE FROM ACTION\n",
    "        \n",
    "        \n",
    "        ONE NET OUTPUT WILL BE A SINGLE VALUE FOR THE STATE\n",
    "        THE OTHER OUTPUT WILL BE ONE VALUE FOR EACH ACTION ???\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def preprocess_input(self, state):\n",
    "        return state\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        # perform the forward pass\n",
    "        # x = self.preprocess_input(state)\n",
    "        x = self.ac1(self.fc1(state))\n",
    "        x = self.ac2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        # x = self.model(state)\n",
    "        return x\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# TODO: IMPROVE PARAMETERS\n",
    "# \"\"\"\n",
    "# BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "# BATCH_SIZE = 64         # minibatch size\n",
    "# GAMMA = 0.99            # discount factor\n",
    "# TAU = 1e-3              # for soft update of target parameters\n",
    "# LR = 5e-4               # learning rate \n",
    "# UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 state_size, \n",
    "                 action_size,\n",
    "                 t_step=0,\n",
    "                 alpha=0.1,\n",
    "                 model_path=None,\n",
    "                 buffer_size=int(1e5),\n",
    "                 batch_size=64,\n",
    "                 gamma=0.99,\n",
    "                 tau=1e-3,\n",
    "                 learning_rate=5e-4,\n",
    "                 update_every=4,\n",
    "                 seed=0):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Q-Network\n",
    "        if not model_path:\n",
    "            self.qnetwork_local = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_target = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate)\n",
    "            self.training = True\n",
    "        else:\n",
    "            print(f\"Loading model from file {model_path}\")\n",
    "            self.qnetwork_local = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_target = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_local.load_state_dict(torch.load(model_path))\n",
    "            self.qnetwork_target.load_state_dict(torch.load(model_path))\n",
    "            self.training = False\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = PrioritizedReplayBuffer(action_size, buffer_size, batch_size, seed)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = t_step\n",
    "        self.alpha = alpha\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_every = update_every\n",
    "        \n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        \n",
    "\n",
    "#         # Compute target values: # reward + gamma * max(Q(s', a')) * (1-done)\n",
    "#         td_targets_next_states = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "#         td_targets = rewards + (gamma*td_targets_next_states * (1-dones))\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: IMPLEMENT DOUBLE DQN: INSTEAD OF CALCULATING THE MAX VALUE USING THE TARGET NETWORK, \n",
    "        WE WILL GET THE INDEX OF THE ACTION USING THE LOCAL NETWORK. WITH THIS INDEX, WE WILL GET THE VALUE CALCULATED\n",
    "        USING THE TARGET NETWORK\n",
    "        \n",
    "        From the paper:\n",
    "        In the original Double Q-learning algorithm, two value\n",
    "        functions are learned by assigning each experience randomly to update one of the two value functions, such that\n",
    "        there are two sets of weights, θ and θ`. For each update, one\n",
    "        set of weights is used to determine the greedy policy and the\n",
    "        other to determine its value. For a clear comparison, we can\n",
    "        first untangle the selection and evaluation in Q-learning and\n",
    "        rewrite its target (2) as\n",
    "        \n",
    "        PSEUDOCODE\n",
    "        td_targets_next_states_indexes = np.argmax(self.qnetwork_local(next_states), axis=1)\n",
    "        td_targets_next_states = self.qnetwork_target(next_states).gather(td_targets_next_states_indexes)\n",
    "        \n",
    "        \"\"\"\n",
    "        td_targets_next_states_indexes = np.argmax(self.qnetwork_local(next_states), axis=1) # Get the action index using the local network\n",
    "        td_targets_next_states = self.qnetwork_target(next_states).gather(td_targets_next_states_indexes) # use the indexes to get the values from the target network\n",
    "        td_targets = rewards + (gamma*td_targets_next_states * (1-dones)) # calculate the target just as before\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(f\"td_targets_next_states: {td_targets_next_states.shape}. Should be {rewards.shape[0]}\")\n",
    "        \n",
    "        # Current Q values\n",
    "        q_values_local = self.qnetwork_local(states).gather(1, actions) # Q\n",
    "        \n",
    "        # compute loss\n",
    "        loss = F.mse_loss(q_values_local, td_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- # # Only update the target network after we pass on the samples\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)                     \n",
    "\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "            \n",
    "    def calculate_error_delta(self, state, action, reward, next_state, done):\n",
    "        # Compute target values: # reward + gamma * max(Q(s', a')) * (1-done)\n",
    "        # VANILA DQN. DO DOUBLE DQN LATER\n",
    "        td_targets_next_state = self.qnetwork_target(next_state).detach().max(1)[0].unsqueeze(1)\n",
    "        td_target = reward + (gamma*td_targets_next_state * (1-dones))\n",
    "        \n",
    "        q_value_local = self.qnetwork_local(state).gather(1, action) # Q\n",
    "        \n",
    "        delta_error = td_target - q_value_local\n",
    "        return delta_error\n",
    "\n",
    "            \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        error = self.calculate_error_delta(state, action, reward, next_state, done)\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done, error)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "\n",
    "                \n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "            \n",
    "        if self.training:\n",
    "            self.qnetwork_local.train()\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"\n",
    "    TODO: IMPLEMENT PRIORITIZED EXPERIENCE REPLAY\n",
    "    \n",
    "    \"\"\"    \n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\", \"error\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done, error):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done, error)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train new agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"./Banana.exe\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.99\n",
    "agent = Agent(state_size=8, \n",
    "              action_size=4,\n",
    "              t_setp=t_step,\n",
    "              alpha=alpha,\n",
    "              gamma=gamma,\n",
    "              seed=0)\n",
    "\n",
    "\n",
    "n_episodes=2000\n",
    "max_t=1000\n",
    "eps_start=1.0\n",
    "eps_end=0.01\n",
    "eps_decay=0.995\n",
    "\n",
    "scores = dqn(agent, env, brain_name, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rainbow DQN: \n",
    "\n",
    "### Implementing Learning from multi-step boostrap targets, Distributional DQN and Noisy DQN on top of the previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
