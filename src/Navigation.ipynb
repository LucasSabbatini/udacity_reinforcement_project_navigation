{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"../banana_unity_env/Banana.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The model will be a simple feed-forward neural network, with 3 fully connected layers each connected to a ReLU activation function, all of them have 64 hidden units.\n",
    "\n",
    "I have tested with a few variations for the network without changing the hyperparameterrs. With and without dropout, more layers, one with a 512 units in the middle, and varying width. None of these changes seem to have a positive effect on the training. They either performed similarly, or worse than the simple one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.ac1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.ac2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def preprocess_input(self, state):\n",
    "        return state\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        # perform the forward pass\n",
    "        # x = self.preprocess_input(state)\n",
    "        x = self.ac1(self.fc1(state))\n",
    "        x = self.ac2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "This part will present a vanilla version of the DQN algorithm, only with Experience Replay Buffer and Target Network. The next part will show the impact of adding the improvements presented in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 state_size, \n",
    "                 action_size,\n",
    "                 t_step=0,\n",
    "                 alpha=0.1,\n",
    "                 model_path=None,\n",
    "                 buffer_size=int(1e5),\n",
    "                 batch_size=64,\n",
    "                 gamma=0.99,\n",
    "                 tau=1e-3,\n",
    "                 learning_rate=5e-4,\n",
    "                 update_every=4,\n",
    "                 seed=0):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Q-Network\n",
    "        if not model_path:\n",
    "            print(f\"Creating new models for local and target networks.\")\n",
    "            self.qnetwork_local = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_target = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate)\n",
    "            self.training = True\n",
    "        else:\n",
    "            print(f\"Loading model from file {model_path}\")\n",
    "            self.qnetwork_local = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_target = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_local.load_state_dict(torch.load(model_path))\n",
    "            self.qnetwork_target.load_state_dict(torch.load(model_path))\n",
    "            self.training = False\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed, self.device)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = t_step\n",
    "        self.alpha = alpha\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_every = update_every\n",
    "        \n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Compute target values: # reward + gamma * max(Q(s', a')) * (1-done)\n",
    "        td_targets_next_states = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        td_targets = rewards + (gamma*td_targets_next_states * (1-dones))\n",
    "#         print(f\"td_targets_next_states: {td_targets_next_states.shape}. Should be {rewards.shape[0]}\")\n",
    "        \n",
    "        # Current Q values\n",
    "        q_values_local = self.qnetwork_local(states).gather(1, actions) # Q\n",
    "        \n",
    "        # compute loss (target - Q)**2\n",
    "        loss = F.mse_loss(q_values_local, td_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- # # Only update the target network after we pass on the samples\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)                     \n",
    "\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "            \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "\n",
    "                \n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "            \n",
    "        if self.training:\n",
    "            self.qnetwork_local.train()\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "\n",
    "class ReplayBuffer: \n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed, device):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training algorithm: Deep Q-Learning\n",
    "\n",
    "This is a functinal implementation of the learning part of the Q-Learning algorithm, fit to match the unity environment's api. The algorithm is pretty simple, in which it receives the agent and the environment, along with some hyperparameters, and iterates over episodes and steps, at each step giving the agent the state, and asking it for the action to be taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def dqn(agent, env, brain_name, model_name=\"dqn_3fc_act_nodrop\", n_episodes=2000, max_t=2000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"\n",
    "    Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    best = 0\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    episode_start = time.time()\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            env_info = env.step(action.astype(np.int32))[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]   \n",
    "            done = env_info.local_done[0]\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tAverage steps per second:{}'.format(i_episode, np.mean(scores_window), t/(time.time()-episode_start)), end=\"\")\n",
    "        episode_start = time.time()\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=5.0 and np.mean(scores_window)>best:\n",
    "            best = np.mean(scores_window)\n",
    "#             print(f\"\\nOverwriting last checkpiont with current average score: {best}\")\n",
    "            filename = model_name + timestr + f\"-score-{int(np.mean(scores_window))}\" + \".pth\"\n",
    "            torch.save(agent.qnetwork_local.state_dict(), filename)\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the environment and the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"../banana_unity_env/Banana.exe\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new models for local and target networks.\n"
     ]
    }
   ],
   "source": [
    "t_step = 0\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.99\n",
    "agent = Agent(state_size=37, \n",
    "              action_size=4,\n",
    "              t_step=t_step,\n",
    "              alpha=alpha,\n",
    "              gamma=gamma,\n",
    "              seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Deep Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The best result so far\n",
    "\n",
    "With a vanilla DQN, the best result I could get with the fixed set of hyperparameters, was with the simple network. It achieved an average score of 17.16 bananas per episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.78\n",
      "Episode 200\tAverage Score: 4.27\n",
      "Episode 300\tAverage Score: 6.86\n",
      "Episode 400\tAverage Score: 10.49\n",
      "Episode 500\tAverage Score: 13.23\n",
      "Episode 600\tAverage Score: 13.57\n",
      "Episode 700\tAverage Score: 14.30\n",
      "Episode 739\tAverage Score: 15.00\n",
      "Environment solved in 639 episodes!\tAverage Score: 15.00\n",
      "Overwriting last checkpiont with current average score: 15.0\n",
      "Episode 740\tAverage Score: 15.09\n",
      "Environment solved in 640 episodes!\tAverage Score: 15.09\n",
      "Overwriting last checkpiont with current average score: 15.09\n",
      "Episode 742\tAverage Score: 15.11\n",
      "Environment solved in 642 episodes!\tAverage Score: 15.11\n",
      "Overwriting last checkpiont with current average score: 15.11\n",
      "Episode 743\tAverage Score: 15.16\n",
      "Environment solved in 643 episodes!\tAverage Score: 15.16\n",
      "Overwriting last checkpiont with current average score: 15.16\n",
      "Episode 745\tAverage Score: 15.19\n",
      "Environment solved in 645 episodes!\tAverage Score: 15.19\n",
      "Overwriting last checkpiont with current average score: 15.19\n",
      "Episode 746\tAverage Score: 15.26\n",
      "Environment solved in 646 episodes!\tAverage Score: 15.26\n",
      "Overwriting last checkpiont with current average score: 15.26\n",
      "Episode 790\tAverage Score: 15.32\n",
      "Environment solved in 690 episodes!\tAverage Score: 15.32\n",
      "Overwriting last checkpiont with current average score: 15.32\n",
      "Episode 791\tAverage Score: 15.33\n",
      "Environment solved in 691 episodes!\tAverage Score: 15.33\n",
      "Overwriting last checkpiont with current average score: 15.33\n",
      "Episode 792\tAverage Score: 15.34\n",
      "Environment solved in 692 episodes!\tAverage Score: 15.34\n",
      "Overwriting last checkpiont with current average score: 15.34\n",
      "Episode 800\tAverage Score: 14.86\n",
      "Episode 900\tAverage Score: 15.17\n",
      "Episode 921\tAverage Score: 15.35\n",
      "Environment solved in 821 episodes!\tAverage Score: 15.35\n",
      "Overwriting last checkpiont with current average score: 15.35\n",
      "Episode 1000\tAverage Score: 14.93\n",
      "Episode 1088\tAverage Score: 15.36\n",
      "Environment solved in 988 episodes!\tAverage Score: 15.36\n",
      "Overwriting last checkpiont with current average score: 15.36\n",
      "Episode 1089\tAverage Score: 15.41\n",
      "Environment solved in 989 episodes!\tAverage Score: 15.41\n",
      "Overwriting last checkpiont with current average score: 15.41\n",
      "Episode 1090\tAverage Score: 15.43\n",
      "Environment solved in 990 episodes!\tAverage Score: 15.43\n",
      "Overwriting last checkpiont with current average score: 15.43\n",
      "Episode 1091\tAverage Score: 15.44\n",
      "Environment solved in 991 episodes!\tAverage Score: 15.44\n",
      "Overwriting last checkpiont with current average score: 15.44\n",
      "Episode 1092\tAverage Score: 15.49\n",
      "Environment solved in 992 episodes!\tAverage Score: 15.49\n",
      "Overwriting last checkpiont with current average score: 15.49\n",
      "Episode 1095\tAverage Score: 15.65\n",
      "Environment solved in 995 episodes!\tAverage Score: 15.65\n",
      "Overwriting last checkpiont with current average score: 15.65\n",
      "Episode 1099\tAverage Score: 15.66\n",
      "Environment solved in 999 episodes!\tAverage Score: 15.66\n",
      "Overwriting last checkpiont with current average score: 15.66\n",
      "Episode 1100\tAverage Score: 15.61\n",
      "Episode 1103\tAverage Score: 15.70\n",
      "Environment solved in 1003 episodes!\tAverage Score: 15.70\n",
      "Overwriting last checkpiont with current average score: 15.7\n",
      "Episode 1104\tAverage Score: 15.72\n",
      "Environment solved in 1004 episodes!\tAverage Score: 15.72\n",
      "Overwriting last checkpiont with current average score: 15.72\n",
      "Episode 1105\tAverage Score: 15.78\n",
      "Environment solved in 1005 episodes!\tAverage Score: 15.78\n",
      "Overwriting last checkpiont with current average score: 15.78\n",
      "Episode 1118\tAverage Score: 15.81\n",
      "Environment solved in 1018 episodes!\tAverage Score: 15.81\n",
      "Overwriting last checkpiont with current average score: 15.81\n",
      "Episode 1141\tAverage Score: 15.88\n",
      "Environment solved in 1041 episodes!\tAverage Score: 15.88\n",
      "Overwriting last checkpiont with current average score: 15.88\n",
      "Episode 1142\tAverage Score: 15.95\n",
      "Environment solved in 1042 episodes!\tAverage Score: 15.95\n",
      "Overwriting last checkpiont with current average score: 15.95\n",
      "Episode 1143\tAverage Score: 15.98\n",
      "Environment solved in 1043 episodes!\tAverage Score: 15.98\n",
      "Overwriting last checkpiont with current average score: 15.98\n",
      "Episode 1144\tAverage Score: 15.99\n",
      "Environment solved in 1044 episodes!\tAverage Score: 15.99\n",
      "Overwriting last checkpiont with current average score: 15.99\n",
      "Episode 1148\tAverage Score: 16.11\n",
      "Environment solved in 1048 episodes!\tAverage Score: 16.11\n",
      "Overwriting last checkpiont with current average score: 16.11\n",
      "Episode 1149\tAverage Score: 16.16\n",
      "Environment solved in 1049 episodes!\tAverage Score: 16.16\n",
      "Overwriting last checkpiont with current average score: 16.16\n",
      "Episode 1150\tAverage Score: 16.18\n",
      "Environment solved in 1050 episodes!\tAverage Score: 16.18\n",
      "Overwriting last checkpiont with current average score: 16.18\n",
      "Episode 1160\tAverage Score: 16.24\n",
      "Environment solved in 1060 episodes!\tAverage Score: 16.24\n",
      "Overwriting last checkpiont with current average score: 16.24\n",
      "Episode 1161\tAverage Score: 16.33\n",
      "Environment solved in 1061 episodes!\tAverage Score: 16.33\n",
      "Overwriting last checkpiont with current average score: 16.33\n",
      "Episode 1162\tAverage Score: 16.39\n",
      "Environment solved in 1062 episodes!\tAverage Score: 16.39\n",
      "Overwriting last checkpiont with current average score: 16.39\n",
      "Episode 1163\tAverage Score: 16.44\n",
      "Environment solved in 1063 episodes!\tAverage Score: 16.44\n",
      "Overwriting last checkpiont with current average score: 16.44\n",
      "Episode 1166\tAverage Score: 16.45\n",
      "Environment solved in 1066 episodes!\tAverage Score: 16.45\n",
      "Overwriting last checkpiont with current average score: 16.45\n",
      "Episode 1171\tAverage Score: 16.52\n",
      "Environment solved in 1071 episodes!\tAverage Score: 16.52\n",
      "Overwriting last checkpiont with current average score: 16.52\n",
      "Episode 1172\tAverage Score: 16.56\n",
      "Environment solved in 1072 episodes!\tAverage Score: 16.56\n",
      "Overwriting last checkpiont with current average score: 16.56\n",
      "Episode 1175\tAverage Score: 16.66\n",
      "Environment solved in 1075 episodes!\tAverage Score: 16.66\n",
      "Overwriting last checkpiont with current average score: 16.66\n",
      "Episode 1200\tAverage Score: 16.50\n",
      "Episode 1223\tAverage Score: 16.69\n",
      "Environment solved in 1123 episodes!\tAverage Score: 16.69\n",
      "Overwriting last checkpiont with current average score: 16.69\n",
      "Episode 1224\tAverage Score: 16.71\n",
      "Environment solved in 1124 episodes!\tAverage Score: 16.71\n",
      "Overwriting last checkpiont with current average score: 16.71\n",
      "Episode 1225\tAverage Score: 16.78\n",
      "Environment solved in 1125 episodes!\tAverage Score: 16.78\n",
      "Overwriting last checkpiont with current average score: 16.78\n",
      "Episode 1226\tAverage Score: 16.92\n",
      "Environment solved in 1126 episodes!\tAverage Score: 16.92\n",
      "Overwriting last checkpiont with current average score: 16.92\n",
      "Episode 1227\tAverage Score: 16.98\n",
      "Environment solved in 1127 episodes!\tAverage Score: 16.98\n",
      "Overwriting last checkpiont with current average score: 16.98\n",
      "Episode 1300\tAverage Score: 15.63\n",
      "Episode 1400\tAverage Score: 17.03\n",
      "\n",
      "Environment solved in 1300 episodes!\tAverage Score: 17.03\n",
      "Overwriting last checkpiont with current average score: 17.03\n",
      "Episode 1420\tAverage Score: 17.16\n",
      "Environment solved in 1320 episodes!\tAverage Score: 17.16\n",
      "Overwriting last checkpiont with current average score: 17.16\n",
      "Episode 1500\tAverage Score: 16.31\n",
      "Episode 1600\tAverage Score: 15.59\n",
      "Episode 1700\tAverage Score: 16.10\n",
      "Episode 1800\tAverage Score: 14.89\n",
      "Episode 1900\tAverage Score: 15.95\n",
      "Episode 2000\tAverage Score: 15.55\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3 FC with act (max=64)\n",
    "scores = dqn(agent, \n",
    "             env, \n",
    "             brain_name, \n",
    "             n_episodes=2000, \n",
    "             max_t=1000, \n",
    "             eps_start=1.0, \n",
    "             eps_end=0.01, \n",
    "             eps_decay=0.995)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# def test_agent(agent, env, brain_name, times=5, max_t=500, freq=1000):\n",
    "def test_agent(model_path, env, brain_name, times=5, max_t=2500, freq=25):\n",
    "    \n",
    "    t_step = 0\n",
    "    alpha = 0.1\n",
    "    gamma = 0.99\n",
    "    epsilon = 0.99\n",
    "    agent = Agent(state_size=37, \n",
    "                  action_size=4,\n",
    "                  t_step=t_step,\n",
    "                  alpha=alpha,\n",
    "                  model_path=model_path,\n",
    "                  seed=0)\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(times):\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        state = env_info.vector_observations[0]         \n",
    "        score = 0                                       \n",
    "        for i in range(max_t):\n",
    "            action = agent.act(state, eps=0) # eps = 0 means the policy is 100% greedy\n",
    "            env_info = env.step(action.astype(np.int32))[brain_name] \n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]                \n",
    "            done = env_info.local_done[0]\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "            time.sleep(1/freq)\n",
    "        \n",
    "        scores.append(score)\n",
    "        print(f\"Episode ended with score: {score}. Starting a new one\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from file ./dqn_3fc_act_nodrop_avg17.pth\n",
      "Episode ended with score: 19.0. Starting a new one\n",
      "Episode ended with score: 16.0. Starting a new one\n",
      "Episode ended with score: 20.0. Starting a new one\n",
      "Episode ended with score: 17.0. Starting a new one\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e3985dfbc3d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./dqn_3fc_act_nodrop_avg17.pth\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-35b7b307767b>\u001b[0m in \u001b[0;36mtest_agent\u001b[1;34m(model_path, env, brain_name, times, max_t, freq)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_path = \"./dqn_3fc_act_nodrop_avg17.pth\"\n",
    "test_agent(model_path, env, brain_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing improvements: Prioritized Experience Replay, Double DQN, Dueling DQN\n",
    "\n",
    "\n",
    "\n",
    "# Prioritized Experience Replay Buffer\n",
    "\n",
    "Prioritized Experience Replay samples the history buffer using a probability for each state, based on the intensity of its error when calculating the target and loss.\n",
    "\n",
    "For this improvement, the method act from the Agent class was altered to also store the delta error, which is calculated in a new method called calculate_delta_error.\n",
    "\n",
    "Also, the add and the sample methods from the PrioritizedReplayBuffer were modified, the former to also add the error to the named tuple, and the latter for using the probability when choosing the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \"\"\"\n",
    "        TODO: IMPROVE MODEL\n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.ac1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.ac2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: IMPLEMENT DUELING DQNS: SEPARATE VALUE FROM ACTION\n",
    "        \n",
    "        \n",
    "        ONE NET OUTPUT WILL BE A SINGLE VALUE FOR THE STATE\n",
    "        THE OTHER OUTPUT WILL BE ONE VALUE FOR EACH ACTION ???\n",
    "        \"\"\"\n",
    "\n",
    "    def preprocess_input(self, state):\n",
    "        return state\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.ac1(self.fc1(state))\n",
    "        x = self.ac2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 state_size, \n",
    "                 action_size,\n",
    "                 t_step=0,\n",
    "                 alpha=0.1,\n",
    "                 model_path=None,\n",
    "                 buffer_size=int(1e5),\n",
    "                 batch_size=64,\n",
    "                 gamma=0.99,\n",
    "                 tau=1e-3,\n",
    "                 learning_rate=5e-4,\n",
    "                 update_every=4,\n",
    "                 seed=0):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Q-Network\n",
    "        if not model_path:\n",
    "            self.qnetwork_local = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_target = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate)\n",
    "            self.training = True\n",
    "        else:\n",
    "            print(f\"Loading model from file {model_path}\")\n",
    "            self.qnetwork_local = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_target = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_local.load_state_dict(torch.load(model_path))\n",
    "            self.qnetwork_target.load_state_dict(torch.load(model_path))\n",
    "            self.training = False\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = PrioritizedReplayBuffer(action_size, buffer_size, batch_size, seed, self.device)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = t_step\n",
    "        self.alpha = alpha\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_every = update_every\n",
    "        \n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        \n",
    "\n",
    "        # Compute target values: # reward + gamma * max(Q(s', a')) * (1-done)\n",
    "        td_targets_next_states = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        td_targets = rewards + (gamma*td_targets_next_states * (1-dones))\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: IMPLEMENT DOUBLE DQN: INSTEAD OF CALCULATING THE MAX VALUE USING THE TARGET NETWORK, \n",
    "        WE WILL GET THE INDEX OF THE ACTION USING THE LOCAL NETWORK. WITH THIS INDEX, WE WILL GET THE VALUE CALCULATED\n",
    "        USING THE TARGET NETWORK\n",
    "        \n",
    "        From the paper:\n",
    "        In the original Double Q-learning algorithm, two value\n",
    "        functions are learned by assigning each experience randomly to update one of the two value functions, such that\n",
    "        there are two sets of weights, θ and θ`. For each update, one\n",
    "        set of weights is used to determine the greedy policy and the\n",
    "        other to determine its value. For a clear comparison, we can\n",
    "        first untangle the selection and evaluation in Q-learning and\n",
    "        rewrite its target (2) as\n",
    "        \n",
    "        PSEUDOCODE\n",
    "        td_targets_next_states_indexes = np.argmax(self.qnetwork_local(next_states), axis=1)\n",
    "        td_targets_next_states = self.qnetwork_target(next_states).gather(td_targets_next_states_indexes)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "#         print(f\"td_targets_next_states: {td_targets_next_states.shape}. Should be {rewards.shape[0]}\")\n",
    "        \n",
    "        \n",
    "        # Current Q values\n",
    "        q_values_local = self.qnetwork_local(states).gather(1, actions) # Q\n",
    "        \n",
    "        # compute loss\n",
    "        loss = F.mse_loss(q_values_local, td_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- # # Only update the target network after we pass on the samples\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)                     \n",
    "\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "            \n",
    "    def calculate_error_delta(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        TODO: compute the error delta for the current state, action, reward and next_stat set\n",
    "        \"\"\"\n",
    "        # Compute target values: # reward + gamma * max(Q(s', a')) * (1-done)\n",
    "        # transform input state to torch structure\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        next_state = torch.from_numpy(next_state).float().unsqueeze(0).to(self.device)\n",
    "        action = np.asarray(action)\n",
    "        action = torch.from_numpy(action).long().unsqueeze(0).to(self.device)\n",
    "        action = action.expand(1,1)\n",
    "#         print(f\"State: {state.size()}. Next state: {next_state.size()}. Action: {action.size()}\")\n",
    "        \n",
    "        # VANILA DQN. DO DOUBLE DQN LATER\n",
    "        td_targets_next_state = self.qnetwork_target(next_state).detach().max(1)[0].unsqueeze(1)\n",
    "        td_target = reward + (gamma*td_targets_next_state * (1-done)) # Target\n",
    "        q_value_local = self.qnetwork_local(state).gather(1, action) # Q current\n",
    "        return abs(td_target - q_value_local)\n",
    "\n",
    "            \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: add error delta to the memory along each entry\n",
    "        \"\"\"\n",
    "        error = self.calculate_error_delta(state, action, reward, next_state, done)\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done, error)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "\n",
    "                \n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "            \n",
    "        if self.training:\n",
    "            self.qnetwork_local.train()\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"\n",
    "    TODO: IMPLEMENT PRIORITIZED EXPERIENCE REPLAY\n",
    "    \n",
    "    \"\"\"    \n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed, device):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\", \"error\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.a = 1\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done, error):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done, error)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        \n",
    "        # Choose samples based on the probability, which is proportional to the error \n",
    "        priorities = np.asarray([e.error for e in self.memory], dtype=np.float64)\n",
    "        with torch.no_grad():\n",
    "            probabilities = priorities / sum(priorities) # It gives out a torch grad error, but this has never been in the graph, so why????? \n",
    "        experiences = random.choices(self.memory, k=self.batch_size, weights=probabilities)\n",
    "        \n",
    "#         # now it is choosing the samples randomly. Change it to use the prioritized formula\n",
    "#         experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train prioritized experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"../banana_unity_env/Banana.exe\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.75\tAverage steps per second:87.40199630528595\n",
      "Episode 200\tAverage Score: 2.33\tAverage steps per second:52.155207561272024\n",
      "Episode 300\tAverage Score: 4.80\tAverage steps per second:38.664785989062846\n",
      "Episode 400\tAverage Score: 5.38\tAverage steps per second:34.786267963992547\n",
      "Episode 500\tAverage Score: 6.22\tAverage steps per second:33.907974001244654\n",
      "Episode 600\tAverage Score: 6.78\tAverage steps per second:34.395216746488586\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d46dd4427e64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0meps_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.995\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_end\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.995\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# plot the scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-83d4a0e63bcd>\u001b[0m in \u001b[0;36mdqn\u001b[1;34m(agent, env, brain_name, model_name, n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-90b4cf779963>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[1;31m# If enough samples are available in memory, get random subset and learn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m                 \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-90b4cf779963>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[1;31m# Choose samples based on the probability, which is proportional to the error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m         \u001b[0mpriorities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m             \u001b[0mprobabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpriorities\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpriorities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads\\miniconda\\envs\\drlnd\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t_step = 0\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.99\n",
    "agent = Agent(state_size=37, \n",
    "              action_size=4,\n",
    "              t_step=t_step,\n",
    "              alpha=alpha,\n",
    "              gamma=gamma,\n",
    "              seed=0)\n",
    "\n",
    "\n",
    "n_episodes=2000\n",
    "max_t=1000\n",
    "eps_start=1.0\n",
    "eps_end=0.01\n",
    "eps_decay=0.995\n",
    "\n",
    "scores = dqn(agent, env, brain_name, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "\n",
    "The Prioritized Experience Replay algorithm, the way it was implemented by myself, is not efficient. It takes between 2 to 3 times longer to compute each step. This can be seen in the Average steps per second variable output\n",
    "\n",
    "The reason for this is probable because at each step, we are computing the error delta, which uses both the local and the target networks, so it runs one state at one network at a time (no parallelization). The architecture already used the local network to get the next action, so I'm not sure what's the impact there. Also, at each batch training, the new code computes more stuff with the new replay buffer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double DQN\n",
    "\n",
    "The double DQN consists on separating the act of choosing the next state (target) action, and evaluating it. So when calculating the target, we will first use the local network to perform a forward pass with the input state, then we choose the action with a simple argmax. \n",
    "\n",
    "Once we have the next action chosen, we use the target network to evaluate it. This modification aims to reduce the overestimation of the action values, which is a known probelm with the DQN algorithm.\n",
    "\n",
    "All necessary changes to accomplish this are present in the learn method of the Agent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.ac1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.ac2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def preprocess_input(self, state):\n",
    "        return state\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        # perform the forward pass\n",
    "        # x = self.preprocess_input(state)\n",
    "        x = self.ac1(self.fc1(state))\n",
    "        x = self.ac2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        # x = self.model(state)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 state_size, \n",
    "                 action_size,\n",
    "                 t_step=0,\n",
    "                 alpha=0.1,\n",
    "                 model_path=None,\n",
    "                 buffer_size=int(1e5),\n",
    "                 batch_size=64,\n",
    "                 gamma=0.99,\n",
    "                 tau=1e-3,\n",
    "                 learning_rate=5e-4,\n",
    "                 update_every=4,\n",
    "                 seed=0):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Q-Network\n",
    "        if not model_path:\n",
    "            print(f\"Creating new models for local and target networks.\")\n",
    "            self.qnetwork_local = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_target = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate)\n",
    "            self.training = True\n",
    "        else:\n",
    "            print(f\"Loading model from file {model_path}\")\n",
    "            self.qnetwork_local = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_target = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_local.load_state_dict(torch.load(model_path))\n",
    "            self.qnetwork_target.load_state_dict(torch.load(model_path))\n",
    "            self.training = False\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed, self.device)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = t_step\n",
    "        self.alpha = alpha\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_every = update_every\n",
    "        \n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "#         # Compute target values: # reward + gamma * max(Q(s', a')) * (1-done)\n",
    "#         td_targets_next_states = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "#         td_targets = rewards + (gamma*td_targets_next_states * (1-dones))\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: IMPLEMENT DOUBLE DQN: INSTEAD OF CALCULATING THE MAX VALUE USING THE TARGET NETWORK, \n",
    "        WE WILL GET THE INDEX OF THE ACTION USING THE LOCAL NETWORK. WITH THIS INDEX, WE WILL GET THE VALUE CALCULATED\n",
    "        USING THE TARGET NETWORK\n",
    "        \n",
    "        From the paper:\n",
    "        \"In the original Double Q-learning algorithm, two value\n",
    "        functions are learned by assigning each experience randomly to update one of the two value functions, such that\n",
    "        there are two sets of weights, θ and θ`. For each update, one\n",
    "        set of weights is used to determine the greedy policy and the\n",
    "        other to determine its value. For a clear comparison, we can\n",
    "        first untangle the selection and evaluation in Q-learning...\"\n",
    "        \n",
    "        PSEUDOCODE\n",
    "        td_targets_next_states_indexes = np.argmax(self.qnetwork_local(next_states), axis=1)\n",
    "        td_targets_next_states = self.qnetwork_target(next_states).gather(td_targets_next_states_indexes)\n",
    "        \n",
    "        \"\"\"\n",
    "        td_targets_next_states_indexes = np.argmax(self.qnetwork_local(next_states).detach(), axis=1).unsqueeze(1) # Get the action index using the local network\n",
    "        td_targets_next_states = self.qnetwork_target(next_states).detach().gather(1, td_targets_next_states_indexes) # use the indexes to get the values from the target network\n",
    "        td_targets = rewards + (gamma*td_targets_next_states * (1-dones)) # calculate the target just as before\n",
    "              \n",
    "        # Current Q values\n",
    "        q_values_local = self.qnetwork_local(states).gather(1, actions) # Q\n",
    "        \n",
    "        # compute loss\n",
    "        loss = F.mse_loss(q_values_local, td_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- # # Only update the target network after we pass on the samples\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)                     \n",
    "\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "            \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "\n",
    "                \n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "            \n",
    "        if self.training:\n",
    "            self.qnetwork_local.train()\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "\n",
    "class ReplayBuffer: \n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed, device):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"../banana_unity_env/Banana.exe\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new models for local and target networks.\n",
      "Episode 100\tAverage Score: 0.19\tAverage steps per second:197.83916004046083\n",
      "Episode 200\tAverage Score: 2.54\tAverage steps per second:194.13369824759113\n",
      "Episode 254\tAverage Score: 5.07\tAverage steps per second:177.95805026969063\n",
      "Overwriting last checkpiont with current average score: 5.07\n",
      "Episode 255\tAverage Score: 5.11\tAverage steps per second:181.14264386583423\n",
      "Overwriting last checkpiont with current average score: 5.11\n",
      "Episode 256\tAverage Score: 5.19\tAverage steps per second:185.10638898327176\n",
      "Overwriting last checkpiont with current average score: 5.19\n",
      "Episode 257\tAverage Score: 5.24\tAverage steps per second:175.65388493542557\n",
      "Overwriting last checkpiont with current average score: 5.24\n",
      "Episode 258\tAverage Score: 5.28\tAverage steps per second:179.31531663437144\n",
      "Overwriting last checkpiont with current average score: 5.28\n",
      "Episode 259\tAverage Score: 5.29\tAverage steps per second:192.47169910065568\n",
      "Overwriting last checkpiont with current average score: 5.29\n",
      "Episode 260\tAverage Score: 5.31\tAverage steps per second:185.6136136918389\n",
      "Overwriting last checkpiont with current average score: 5.31\n",
      "Episode 261\tAverage Score: 5.32\tAverage steps per second:151.55872217106202\n",
      "Overwriting last checkpiont with current average score: 5.32\n",
      "Episode 262\tAverage Score: 5.34\tAverage steps per second:190.66450049608468\n",
      "Overwriting last checkpiont with current average score: 5.34\n",
      "Episode 264\tAverage Score: 5.40\tAverage steps per second:191.52043811114962\n",
      "Overwriting last checkpiont with current average score: 5.4\n",
      "Episode 265\tAverage Score: 5.46\tAverage steps per second:184.59423919121588\n",
      "Overwriting last checkpiont with current average score: 5.46\n",
      "Episode 266\tAverage Score: 5.47\tAverage steps per second:190.29063145965156\n",
      "Overwriting last checkpiont with current average score: 5.47\n",
      "Episode 267\tAverage Score: 5.52\tAverage steps per second:188.8688766935139\n",
      "Overwriting last checkpiont with current average score: 5.52\n",
      "Episode 269\tAverage Score: 5.62\tAverage steps per second:186.51326209097045\n",
      "Overwriting last checkpiont with current average score: 5.62\n",
      "Episode 270\tAverage Score: 5.70\tAverage steps per second:179.28393980184794\n",
      "Overwriting last checkpiont with current average score: 5.7\n",
      "Episode 271\tAverage Score: 5.76\tAverage steps per second:165.641992891681\n",
      "Overwriting last checkpiont with current average score: 5.76\n",
      "Episode 272\tAverage Score: 5.82\tAverage steps per second:173.94138988682477\n",
      "Overwriting last checkpiont with current average score: 5.82\n",
      "Episode 273\tAverage Score: 5.83\tAverage steps per second:184.10221081744461\n",
      "Overwriting last checkpiont with current average score: 5.83\n",
      "Episode 274\tAverage Score: 5.89\tAverage steps per second:175.17499677683355\n",
      "Overwriting last checkpiont with current average score: 5.89\n",
      "Episode 275\tAverage Score: 5.98\tAverage steps per second:183.00757087894488\n",
      "Overwriting last checkpiont with current average score: 5.98\n",
      "Episode 276\tAverage Score: 6.00\tAverage steps per second:184.97339936992958\n",
      "Overwriting last checkpiont with current average score: 6.0\n",
      "Episode 279\tAverage Score: 6.05\tAverage steps per second:191.51815677863867\n",
      "Overwriting last checkpiont with current average score: 6.05\n",
      "Episode 280\tAverage Score: 6.07\tAverage steps per second:187.721436326261\n",
      "Overwriting last checkpiont with current average score: 6.07\n",
      "Episode 281\tAverage Score: 6.10\tAverage steps per second:170.58038088172907\n",
      "Overwriting last checkpiont with current average score: 6.1\n",
      "Episode 283\tAverage Score: 6.12\tAverage steps per second:182.45878641401035\n",
      "Overwriting last checkpiont with current average score: 6.12\n",
      "Episode 284\tAverage Score: 6.13\tAverage steps per second:182.40740790301214\n",
      "Overwriting last checkpiont with current average score: 6.13\n",
      "Episode 285\tAverage Score: 6.21\tAverage steps per second:176.50643016444792\n",
      "Overwriting last checkpiont with current average score: 6.21\n",
      "Episode 286\tAverage Score: 6.33\tAverage steps per second:171.65000339441082\n",
      "Overwriting last checkpiont with current average score: 6.33\n",
      "Episode 287\tAverage Score: 6.34\tAverage steps per second:170.93359688330622\n",
      "Overwriting last checkpiont with current average score: 6.34\n",
      "Episode 288\tAverage Score: 6.40\tAverage steps per second:175.38151216129265\n",
      "Overwriting last checkpiont with current average score: 6.4\n",
      "Episode 289\tAverage Score: 6.51\tAverage steps per second:180.63577092602384\n",
      "Overwriting last checkpiont with current average score: 6.51\n",
      "Episode 290\tAverage Score: 6.57\tAverage steps per second:191.54056298799742\n",
      "Overwriting last checkpiont with current average score: 6.57\n",
      "Episode 291\tAverage Score: 6.59\tAverage steps per second:194.32423328820627\n",
      "Overwriting last checkpiont with current average score: 6.59\n",
      "Episode 296\tAverage Score: 6.67\tAverage steps per second:167.24137279889217\n",
      "Overwriting last checkpiont with current average score: 6.67\n",
      "Episode 297\tAverage Score: 6.71\tAverage steps per second:147.62487063221883\n",
      "Overwriting last checkpiont with current average score: 6.71\n",
      "Episode 298\tAverage Score: 6.74\tAverage steps per second:166.54162349070035\n",
      "Overwriting last checkpiont with current average score: 6.74\n",
      "Episode 299\tAverage Score: 6.84\tAverage steps per second:174.03156916332253\n",
      "Overwriting last checkpiont with current average score: 6.84\n",
      "Episode 300\tAverage Score: 6.86\tAverage steps per second:165.91041212454755\n",
      "\n",
      "Overwriting last checkpiont with current average score: 6.86\n",
      "Episode 301\tAverage Score: 6.91\tAverage steps per second:176.01636629235435\n",
      "Overwriting last checkpiont with current average score: 6.91\n",
      "Episode 302\tAverage Score: 6.96\tAverage steps per second:160.1416820038791\n",
      "Overwriting last checkpiont with current average score: 6.96\n",
      "Episode 303\tAverage Score: 7.03\tAverage steps per second:181.9650967416545\n",
      "Overwriting last checkpiont with current average score: 7.03\n",
      "Episode 304\tAverage Score: 7.05\tAverage steps per second:181.70229669026006\n",
      "Overwriting last checkpiont with current average score: 7.05\n",
      "Episode 305\tAverage Score: 7.13\tAverage steps per second:189.1160076744404\n",
      "Overwriting last checkpiont with current average score: 7.13\n",
      "Episode 307\tAverage Score: 7.16\tAverage steps per second:172.08387988155945\n",
      "Overwriting last checkpiont with current average score: 7.16\n",
      "Episode 310\tAverage Score: 7.22\tAverage steps per second:180.68326653563583\n",
      "Overwriting last checkpiont with current average score: 7.22\n",
      "Episode 311\tAverage Score: 7.27\tAverage steps per second:193.32129551640236\n",
      "Overwriting last checkpiont with current average score: 7.27\n",
      "Episode 312\tAverage Score: 7.33\tAverage steps per second:188.61623472273902\n",
      "Overwriting last checkpiont with current average score: 7.33\n",
      "Episode 314\tAverage Score: 7.34\tAverage steps per second:180.72388532672986\n",
      "Overwriting last checkpiont with current average score: 7.34\n",
      "Episode 315\tAverage Score: 7.44\tAverage steps per second:185.25453779461913\n",
      "Overwriting last checkpiont with current average score: 7.44\n",
      "Episode 316\tAverage Score: 7.48\tAverage steps per second:192.40803307444128\n",
      "Overwriting last checkpiont with current average score: 7.48\n",
      "Episode 317\tAverage Score: 7.50\tAverage steps per second:190.64687781330636\n",
      "Overwriting last checkpiont with current average score: 7.5\n",
      "Episode 318\tAverage Score: 7.51\tAverage steps per second:192.28569176869235\n",
      "Overwriting last checkpiont with current average score: 7.51\n",
      "Episode 319\tAverage Score: 7.57\tAverage steps per second:186.70257748749307\n",
      "Overwriting last checkpiont with current average score: 7.57\n",
      "Episode 320\tAverage Score: 7.65\tAverage steps per second:187.04648029287964\n",
      "Overwriting last checkpiont with current average score: 7.65\n",
      "Episode 322\tAverage Score: 7.68\tAverage steps per second:191.32706894931414\n",
      "Overwriting last checkpiont with current average score: 7.68\n",
      "Episode 323\tAverage Score: 7.69\tAverage steps per second:191.91510601036566\n",
      "Overwriting last checkpiont with current average score: 7.69\n",
      "Episode 324\tAverage Score: 7.76\tAverage steps per second:194.58476987551572\n",
      "Overwriting last checkpiont with current average score: 7.76\n",
      "Episode 325\tAverage Score: 7.80\tAverage steps per second:190.87355436629855\n",
      "Overwriting last checkpiont with current average score: 7.8\n",
      "Episode 326\tAverage Score: 7.84\tAverage steps per second:192.80693460967606\n",
      "Overwriting last checkpiont with current average score: 7.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 327\tAverage Score: 7.86\tAverage steps per second:187.79398884044824\n",
      "Overwriting last checkpiont with current average score: 7.86\n",
      "Episode 328\tAverage Score: 7.94\tAverage steps per second:193.8969985548523\n",
      "Overwriting last checkpiont with current average score: 7.94\n",
      "Episode 329\tAverage Score: 8.04\tAverage steps per second:194.8857925403669\n",
      "Overwriting last checkpiont with current average score: 8.04\n",
      "Episode 330\tAverage Score: 8.05\tAverage steps per second:191.16124317764093\n",
      "Overwriting last checkpiont with current average score: 8.05\n",
      "Episode 331\tAverage Score: 8.08\tAverage steps per second:194.14647111094598\n",
      "Overwriting last checkpiont with current average score: 8.08\n",
      "Episode 332\tAverage Score: 8.13\tAverage steps per second:186.08093034463562\n",
      "Overwriting last checkpiont with current average score: 8.13\n",
      "Episode 333\tAverage Score: 8.17\tAverage steps per second:192.72687394730696\n",
      "Overwriting last checkpiont with current average score: 8.17\n",
      "Episode 338\tAverage Score: 8.26\tAverage steps per second:195.71429668307064\n",
      "Overwriting last checkpiont with current average score: 8.26\n",
      "Episode 346\tAverage Score: 8.33\tAverage steps per second:196.33389588406988\n",
      "Overwriting last checkpiont with current average score: 8.33\n",
      "Episode 348\tAverage Score: 8.36\tAverage steps per second:193.42523705099586\n",
      "Overwriting last checkpiont with current average score: 8.36\n",
      "Episode 349\tAverage Score: 8.39\tAverage steps per second:192.95288589980333\n",
      "Overwriting last checkpiont with current average score: 8.39\n",
      "Episode 350\tAverage Score: 8.44\tAverage steps per second:193.86735427062658\n",
      "Overwriting last checkpiont with current average score: 8.44\n",
      "Episode 352\tAverage Score: 8.45\tAverage steps per second:194.64000604672148\n",
      "Overwriting last checkpiont with current average score: 8.45\n",
      "Episode 355\tAverage Score: 8.47\tAverage steps per second:188.05906637837174\n",
      "Overwriting last checkpiont with current average score: 8.47\n",
      "Episode 356\tAverage Score: 8.52\tAverage steps per second:186.73385230612635\n",
      "Overwriting last checkpiont with current average score: 8.52\n",
      "Episode 357\tAverage Score: 8.54\tAverage steps per second:189.09809980140201\n",
      "Overwriting last checkpiont with current average score: 8.54\n",
      "Episode 358\tAverage Score: 8.59\tAverage steps per second:190.86065661341954\n",
      "Overwriting last checkpiont with current average score: 8.59\n",
      "Episode 359\tAverage Score: 8.69\tAverage steps per second:192.51238354591464\n",
      "Overwriting last checkpiont with current average score: 8.69\n",
      "Episode 360\tAverage Score: 8.72\tAverage steps per second:193.3066048256637\n",
      "Overwriting last checkpiont with current average score: 8.72\n",
      "Episode 361\tAverage Score: 8.73\tAverage steps per second:195.38784700475188\n",
      "Overwriting last checkpiont with current average score: 8.73\n",
      "Episode 362\tAverage Score: 8.79\tAverage steps per second:192.22170677541425\n",
      "Overwriting last checkpiont with current average score: 8.79\n",
      "Episode 363\tAverage Score: 8.82\tAverage steps per second:192.03974027330318\n",
      "Overwriting last checkpiont with current average score: 8.82\n",
      "Episode 365\tAverage Score: 8.85\tAverage steps per second:195.34304760207678\n",
      "Overwriting last checkpiont with current average score: 8.85\n",
      "Episode 366\tAverage Score: 8.87\tAverage steps per second:193.49184696995738\n",
      "Overwriting last checkpiont with current average score: 8.87\n",
      "Episode 367\tAverage Score: 8.88\tAverage steps per second:190.10296387640625\n",
      "Overwriting last checkpiont with current average score: 8.88\n",
      "Episode 377\tAverage Score: 8.93\tAverage steps per second:193.75015484137893\n",
      "Overwriting last checkpiont with current average score: 8.93\n",
      "Episode 378\tAverage Score: 9.00\tAverage steps per second:194.98044920086735\n",
      "Overwriting last checkpiont with current average score: 9.0\n",
      "Episode 380\tAverage Score: 9.04\tAverage steps per second:193.6474187666292\n",
      "Overwriting last checkpiont with current average score: 9.04\n",
      "Episode 381\tAverage Score: 9.10\tAverage steps per second:193.53409879877697\n",
      "Overwriting last checkpiont with current average score: 9.1\n",
      "Episode 382\tAverage Score: 9.21\tAverage steps per second:192.86528862198094\n",
      "Overwriting last checkpiont with current average score: 9.21\n",
      "Episode 383\tAverage Score: 9.33\tAverage steps per second:194.09914480704956\n",
      "Overwriting last checkpiont with current average score: 9.33\n",
      "Episode 384\tAverage Score: 9.43\tAverage steps per second:191.9134613677751\n",
      "Overwriting last checkpiont with current average score: 9.43\n",
      "Episode 385\tAverage Score: 9.46\tAverage steps per second:185.6216907793407\n",
      "Overwriting last checkpiont with current average score: 9.46\n",
      "Episode 387\tAverage Score: 9.47\tAverage steps per second:196.07242799898063\n",
      "Overwriting last checkpiont with current average score: 9.47\n",
      "Episode 388\tAverage Score: 9.53\tAverage steps per second:195.14689675830653\n",
      "Overwriting last checkpiont with current average score: 9.53\n",
      "Episode 392\tAverage Score: 9.54\tAverage steps per second:195.39996341597563\n",
      "Overwriting last checkpiont with current average score: 9.54\n",
      "Episode 393\tAverage Score: 9.60\tAverage steps per second:193.19237377340409\n",
      "Overwriting last checkpiont with current average score: 9.6\n",
      "Episode 400\tAverage Score: 9.66\tAverage steps per second:188.42927724324326\n",
      "\n",
      "Overwriting last checkpiont with current average score: 9.66\n",
      "Episode 401\tAverage Score: 9.75\tAverage steps per second:193.7669488173404\n",
      "Overwriting last checkpiont with current average score: 9.75\n",
      "Episode 402\tAverage Score: 9.82\tAverage steps per second:194.54356702353502\n",
      "Overwriting last checkpiont with current average score: 9.82\n",
      "Episode 403\tAverage Score: 9.87\tAverage steps per second:193.5196743688953\n",
      "Overwriting last checkpiont with current average score: 9.87\n",
      "Episode 406\tAverage Score: 9.97\tAverage steps per second:193.01771516387925\n",
      "Overwriting last checkpiont with current average score: 9.97\n",
      "Episode 408\tAverage Score: 10.08\tAverage steps per second:193.1735070836366\n",
      "Overwriting last checkpiont with current average score: 10.08\n",
      "Episode 413\tAverage Score: 10.09\tAverage steps per second:192.50003162036893\n",
      "Overwriting last checkpiont with current average score: 10.09\n",
      "Episode 414\tAverage Score: 10.16\tAverage steps per second:193.24244490434853\n",
      "Overwriting last checkpiont with current average score: 10.16\n",
      "Episode 415\tAverage Score: 10.17\tAverage steps per second:194.52096569250318\n",
      "Overwriting last checkpiont with current average score: 10.17\n",
      "Episode 416\tAverage Score: 10.24\tAverage steps per second:193.57831127241047\n",
      "Overwriting last checkpiont with current average score: 10.24\n",
      "Episode 417\tAverage Score: 10.26\tAverage steps per second:191.63472048934898\n",
      "Overwriting last checkpiont with current average score: 10.26\n",
      "Episode 418\tAverage Score: 10.28\tAverage steps per second:193.89804781088392\n",
      "Overwriting last checkpiont with current average score: 10.28\n",
      "Episode 419\tAverage Score: 10.36\tAverage steps per second:164.09846193785572\n",
      "Overwriting last checkpiont with current average score: 10.36\n",
      "Episode 420\tAverage Score: 10.39\tAverage steps per second:176.28635463487646\n",
      "Overwriting last checkpiont with current average score: 10.39\n",
      "Episode 421\tAverage Score: 10.40\tAverage steps per second:181.21834285017377\n",
      "Overwriting last checkpiont with current average score: 10.4\n",
      "Episode 422\tAverage Score: 10.49\tAverage steps per second:186.29064651080364\n",
      "Overwriting last checkpiont with current average score: 10.49\n",
      "Episode 423\tAverage Score: 10.52\tAverage steps per second:176.40820912801155\n",
      "Overwriting last checkpiont with current average score: 10.52\n",
      "Episode 424\tAverage Score: 10.54\tAverage steps per second:169.00980922369172\n",
      "Overwriting last checkpiont with current average score: 10.54\n",
      "Episode 425\tAverage Score: 10.58\tAverage steps per second:183.27568964207006\n",
      "Overwriting last checkpiont with current average score: 10.58\n",
      "Episode 426\tAverage Score: 10.64\tAverage steps per second:185.83912939168468\n",
      "Overwriting last checkpiont with current average score: 10.64\n",
      "Episode 427\tAverage Score: 10.65\tAverage steps per second:185.93498590176097\n",
      "Overwriting last checkpiont with current average score: 10.65\n",
      "Episode 428\tAverage Score: 10.66\tAverage steps per second:188.48631423050395\n",
      "Overwriting last checkpiont with current average score: 10.66\n",
      "Episode 431\tAverage Score: 10.76\tAverage steps per second:186.14605844653374\n",
      "Overwriting last checkpiont with current average score: 10.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 432\tAverage Score: 10.84\tAverage steps per second:188.41220684559536\n",
      "Overwriting last checkpiont with current average score: 10.84\n",
      "Episode 433\tAverage Score: 10.90\tAverage steps per second:191.072120648303\n",
      "Overwriting last checkpiont with current average score: 10.9\n",
      "Episode 434\tAverage Score: 10.95\tAverage steps per second:185.39448217586042\n",
      "Overwriting last checkpiont with current average score: 10.95\n",
      "Episode 435\tAverage Score: 10.99\tAverage steps per second:184.4719150706323\n",
      "Overwriting last checkpiont with current average score: 10.99\n",
      "Episode 436\tAverage Score: 11.03\tAverage steps per second:188.80163073465107\n",
      "Overwriting last checkpiont with current average score: 11.03\n",
      "Episode 437\tAverage Score: 11.05\tAverage steps per second:178.55059667454563\n",
      "Overwriting last checkpiont with current average score: 11.05\n",
      "Episode 438\tAverage Score: 11.08\tAverage steps per second:187.2728646009806\n",
      "Overwriting last checkpiont with current average score: 11.08\n",
      "Episode 439\tAverage Score: 11.15\tAverage steps per second:177.62232168996138\n",
      "Overwriting last checkpiont with current average score: 11.15\n",
      "Episode 440\tAverage Score: 11.25\tAverage steps per second:180.13258055031565\n",
      "Overwriting last checkpiont with current average score: 11.25\n",
      "Episode 441\tAverage Score: 11.36\tAverage steps per second:184.32837429349925\n",
      "Overwriting last checkpiont with current average score: 11.36\n",
      "Episode 442\tAverage Score: 11.37\tAverage steps per second:189.86304967653862\n",
      "Overwriting last checkpiont with current average score: 11.37\n",
      "Episode 443\tAverage Score: 11.45\tAverage steps per second:185.60586694182757\n",
      "Overwriting last checkpiont with current average score: 11.45\n",
      "Episode 444\tAverage Score: 11.47\tAverage steps per second:193.5283944711327\n",
      "Overwriting last checkpiont with current average score: 11.47\n",
      "Episode 445\tAverage Score: 11.51\tAverage steps per second:186.41298463817049\n",
      "Overwriting last checkpiont with current average score: 11.51\n",
      "Episode 446\tAverage Score: 11.53\tAverage steps per second:184.6613487129333\n",
      "Overwriting last checkpiont with current average score: 11.53\n",
      "Episode 447\tAverage Score: 11.60\tAverage steps per second:186.15288323935616\n",
      "Overwriting last checkpiont with current average score: 11.6\n",
      "Episode 448\tAverage Score: 11.62\tAverage steps per second:187.33882987408188\n",
      "Overwriting last checkpiont with current average score: 11.62\n",
      "Episode 449\tAverage Score: 11.66\tAverage steps per second:189.51109853845946\n",
      "Overwriting last checkpiont with current average score: 11.66\n",
      "Episode 450\tAverage Score: 11.69\tAverage steps per second:188.51034013566584\n",
      "Overwriting last checkpiont with current average score: 11.69\n",
      "Episode 451\tAverage Score: 11.74\tAverage steps per second:193.27961315007403\n",
      "Overwriting last checkpiont with current average score: 11.74\n",
      "Episode 452\tAverage Score: 11.76\tAverage steps per second:187.2445400933642\n",
      "Overwriting last checkpiont with current average score: 11.76\n",
      "Episode 453\tAverage Score: 11.91\tAverage steps per second:187.07569371899373\n",
      "Overwriting last checkpiont with current average score: 11.91\n",
      "Episode 454\tAverage Score: 11.95\tAverage steps per second:190.09385818426492\n",
      "Overwriting last checkpiont with current average score: 11.95\n",
      "Episode 458\tAverage Score: 11.96\tAverage steps per second:187.82838724345396\n",
      "Overwriting last checkpiont with current average score: 11.96\n",
      "Episode 460\tAverage Score: 11.98\tAverage steps per second:189.11980070062415\n",
      "Overwriting last checkpiont with current average score: 11.98\n",
      "Episode 461\tAverage Score: 12.09\tAverage steps per second:184.1257807856836\n",
      "Overwriting last checkpiont with current average score: 12.09\n",
      "Episode 463\tAverage Score: 12.21\tAverage steps per second:186.41351111111112\n",
      "Overwriting last checkpiont with current average score: 12.21\n",
      "Episode 464\tAverage Score: 12.25\tAverage steps per second:177.65495670030424\n",
      "Overwriting last checkpiont with current average score: 12.25\n",
      "Episode 465\tAverage Score: 12.27\tAverage steps per second:184.09237376261024\n",
      "Overwriting last checkpiont with current average score: 12.27\n",
      "Episode 466\tAverage Score: 12.34\tAverage steps per second:192.80687532477916\n",
      "Overwriting last checkpiont with current average score: 12.34\n",
      "Episode 467\tAverage Score: 12.48\tAverage steps per second:190.67139973618225\n",
      "Overwriting last checkpiont with current average score: 12.48\n",
      "Episode 468\tAverage Score: 12.56\tAverage steps per second:190.84108092314904\n",
      "Overwriting last checkpiont with current average score: 12.56\n",
      "Episode 469\tAverage Score: 12.61\tAverage steps per second:192.2649089226868\n",
      "Overwriting last checkpiont with current average score: 12.61\n",
      "Episode 470\tAverage Score: 12.69\tAverage steps per second:188.50404974318684\n",
      "Overwriting last checkpiont with current average score: 12.69\n",
      "Episode 472\tAverage Score: 12.70\tAverage steps per second:183.84368923200475\n",
      "Overwriting last checkpiont with current average score: 12.7\n",
      "Episode 473\tAverage Score: 12.78\tAverage steps per second:184.7976476222403\n",
      "Overwriting last checkpiont with current average score: 12.78\n",
      "Episode 474\tAverage Score: 12.82\tAverage steps per second:188.63694555702563\n",
      "Overwriting last checkpiont with current average score: 12.82\n",
      "Episode 475\tAverage Score: 12.83\tAverage steps per second:188.8824169779823\n",
      "Overwriting last checkpiont with current average score: 12.83\n",
      "Episode 476\tAverage Score: 12.91\tAverage steps per second:171.38031752105363\n",
      "Overwriting last checkpiont with current average score: 12.91\n",
      "Episode 481\tAverage Score: 12.93\tAverage steps per second:188.78000278782736\n",
      "Overwriting last checkpiont with current average score: 12.93\n",
      "Episode 483\tAverage Score: 12.94\tAverage steps per second:192.38967343072244\n",
      "Overwriting last checkpiont with current average score: 12.94\n",
      "Episode 484\tAverage Score: 12.98\tAverage steps per second:189.9422729153553\n",
      "Overwriting last checkpiont with current average score: 12.98\n",
      "Episode 487\tAverage Score: 13.00\tAverage steps per second:193.73076007495905\n",
      "Overwriting last checkpiont with current average score: 13.0\n",
      "Episode 489\tAverage Score: 13.05\tAverage steps per second:184.95206675339372\n",
      "Overwriting last checkpiont with current average score: 13.05\n",
      "Episode 490\tAverage Score: 13.17\tAverage steps per second:186.04347057899406\n",
      "Overwriting last checkpiont with current average score: 13.17\n",
      "Episode 492\tAverage Score: 13.25\tAverage steps per second:190.68502575265352\n",
      "Overwriting last checkpiont with current average score: 13.25\n",
      "Episode 493\tAverage Score: 13.31\tAverage steps per second:191.28285757559627\n",
      "Overwriting last checkpiont with current average score: 13.31\n",
      "Episode 494\tAverage Score: 13.40\tAverage steps per second:190.0343468877158\n",
      "Overwriting last checkpiont with current average score: 13.4\n",
      "Episode 497\tAverage Score: 13.49\tAverage steps per second:184.42587851038346\n",
      "Overwriting last checkpiont with current average score: 13.49\n",
      "Episode 498\tAverage Score: 13.55\tAverage steps per second:186.30714088024956\n",
      "Overwriting last checkpiont with current average score: 13.55\n",
      "Episode 499\tAverage Score: 13.59\tAverage steps per second:191.55127067799634\n",
      "Overwriting last checkpiont with current average score: 13.59\n",
      "Episode 500\tAverage Score: 13.58\tAverage steps per second:183.7569765843712\n",
      "Episode 504\tAverage Score: 13.62\tAverage steps per second:192.28171172697492\n",
      "Overwriting last checkpiont with current average score: 13.62\n",
      "Episode 505\tAverage Score: 13.63\tAverage steps per second:185.97576659027024\n",
      "Overwriting last checkpiont with current average score: 13.63\n",
      "Episode 509\tAverage Score: 13.68\tAverage steps per second:191.91428368554688\n",
      "Overwriting last checkpiont with current average score: 13.68\n",
      "Episode 510\tAverage Score: 13.78\tAverage steps per second:192.476632318228\n",
      "Overwriting last checkpiont with current average score: 13.78\n",
      "Episode 511\tAverage Score: 13.86\tAverage steps per second:193.3773969466002\n",
      "Overwriting last checkpiont with current average score: 13.86\n",
      "Episode 513\tAverage Score: 13.90\tAverage steps per second:183.12076699186343\n",
      "Overwriting last checkpiont with current average score: 13.9\n",
      "Episode 514\tAverage Score: 13.92\tAverage steps per second:182.51051260826083\n",
      "Overwriting last checkpiont with current average score: 13.92\n",
      "Episode 515\tAverage Score: 13.97\tAverage steps per second:169.6149154378087\n",
      "Overwriting last checkpiont with current average score: 13.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 525\tAverage Score: 14.04\tAverage steps per second:183.87555020946859\n",
      "Overwriting last checkpiont with current average score: 14.04\n",
      "Episode 534\tAverage Score: 14.08\tAverage steps per second:185.76854701558632\n",
      "Overwriting last checkpiont with current average score: 14.08\n",
      "Episode 547\tAverage Score: 14.10\tAverage steps per second:191.02619385192992\n",
      "Overwriting last checkpiont with current average score: 14.1\n",
      "Episode 548\tAverage Score: 14.12\tAverage steps per second:186.16490382816912\n",
      "Overwriting last checkpiont with current average score: 14.12\n",
      "Episode 550\tAverage Score: 14.13\tAverage steps per second:193.29206532643724\n",
      "Overwriting last checkpiont with current average score: 14.13\n",
      "Episode 551\tAverage Score: 14.18\tAverage steps per second:189.19826204499373\n",
      "Overwriting last checkpiont with current average score: 14.18\n",
      "Episode 552\tAverage Score: 14.25\tAverage steps per second:189.27598864914776\n",
      "Overwriting last checkpiont with current average score: 14.25\n",
      "Episode 554\tAverage Score: 14.31\tAverage steps per second:184.91094306583722\n",
      "Overwriting last checkpiont with current average score: 14.31\n",
      "Episode 555\tAverage Score: 14.33\tAverage steps per second:188.39785651652238\n",
      "Overwriting last checkpiont with current average score: 14.33\n",
      "Episode 557\tAverage Score: 14.35\tAverage steps per second:188.74633444259644\n",
      "Overwriting last checkpiont with current average score: 14.35\n",
      "Episode 558\tAverage Score: 14.36\tAverage steps per second:187.81105989886242\n",
      "Overwriting last checkpiont with current average score: 14.36\n",
      "Episode 559\tAverage Score: 14.44\tAverage steps per second:184.91208817295464\n",
      "Overwriting last checkpiont with current average score: 14.44\n",
      "Episode 600\tAverage Score: 14.25\tAverage steps per second:173.93644431752354\n",
      "Episode 621\tAverage Score: 14.46\tAverage steps per second:176.54327883109787\n",
      "Overwriting last checkpiont with current average score: 14.46\n",
      "Episode 622\tAverage Score: 14.48\tAverage steps per second:184.17118078822995\n",
      "Overwriting last checkpiont with current average score: 14.48\n",
      "Episode 626\tAverage Score: 14.51\tAverage steps per second:193.77623015994425\n",
      "Overwriting last checkpiont with current average score: 14.51\n",
      "Episode 628\tAverage Score: 14.52\tAverage steps per second:164.70163675036997\n",
      "Overwriting last checkpiont with current average score: 14.52\n",
      "Episode 629\tAverage Score: 14.60\tAverage steps per second:187.3157731144965\n",
      "Overwriting last checkpiont with current average score: 14.6\n",
      "Episode 632\tAverage Score: 14.64\tAverage steps per second:184.12824084344774\n",
      "Overwriting last checkpiont with current average score: 14.64\n",
      "Episode 638\tAverage Score: 14.65\tAverage steps per second:187.18529046255674\n",
      "Overwriting last checkpiont with current average score: 14.65\n",
      "Episode 665\tAverage Score: 14.68\tAverage steps per second:183.74709562095933\n",
      "Overwriting last checkpiont with current average score: 14.68\n",
      "Episode 666\tAverage Score: 14.73\tAverage steps per second:181.06436774074973\n",
      "Overwriting last checkpiont with current average score: 14.73\n",
      "Episode 668\tAverage Score: 14.75\tAverage steps per second:191.57488448728424\n",
      "Overwriting last checkpiont with current average score: 14.75\n",
      "Episode 670\tAverage Score: 14.78\tAverage steps per second:178.72563872021237\n",
      "Overwriting last checkpiont with current average score: 14.78\n",
      "Episode 671\tAverage Score: 14.81\tAverage steps per second:184.2684645804308\n",
      "Overwriting last checkpiont with current average score: 14.81\n",
      "Episode 675\tAverage Score: 14.89\tAverage steps per second:189.55489570827368\n",
      "Overwriting last checkpiont with current average score: 14.89\n",
      "Episode 676\tAverage Score: 14.91\tAverage steps per second:188.23636097282161\n",
      "Overwriting last checkpiont with current average score: 14.91\n",
      "Episode 677\tAverage Score: 14.96\tAverage steps per second:187.81989194451205\n",
      "Overwriting last checkpiont with current average score: 14.96\n",
      "Episode 683\tAverage Score: 14.99\tAverage steps per second:181.51168387565528\n",
      "Overwriting last checkpiont with current average score: 14.99\n",
      "Episode 684\tAverage Score: 15.00\tAverage steps per second:182.5281508270477\n",
      "Overwriting last checkpiont with current average score: 15.0\n",
      "Episode 685\tAverage Score: 15.01\tAverage steps per second:176.9138862720635\n",
      "Overwriting last checkpiont with current average score: 15.01\n",
      "Episode 686\tAverage Score: 15.12\tAverage steps per second:173.68954796563483\n",
      "Overwriting last checkpiont with current average score: 15.12\n",
      "Episode 687\tAverage Score: 15.15\tAverage steps per second:191.132895529558\n",
      "Overwriting last checkpiont with current average score: 15.15\n",
      "Episode 689\tAverage Score: 15.17\tAverage steps per second:195.58744157393616\n",
      "Overwriting last checkpiont with current average score: 15.17\n",
      "Episode 698\tAverage Score: 15.20\tAverage steps per second:194.12029606165925\n",
      "Overwriting last checkpiont with current average score: 15.2\n",
      "Episode 700\tAverage Score: 15.14\tAverage steps per second:196.32572022020597\n",
      "Episode 704\tAverage Score: 15.21\tAverage steps per second:181.31939951770454\n",
      "Overwriting last checkpiont with current average score: 15.21\n",
      "Episode 706\tAverage Score: 15.26\tAverage steps per second:189.10115074389117\n",
      "Overwriting last checkpiont with current average score: 15.26\n",
      "Episode 709\tAverage Score: 15.28\tAverage steps per second:175.49922046022863\n",
      "Overwriting last checkpiont with current average score: 15.28\n",
      "Episode 710\tAverage Score: 15.32\tAverage steps per second:170.97638322363196\n",
      "Overwriting last checkpiont with current average score: 15.32\n",
      "Episode 765\tAverage Score: 15.35\tAverage steps per second:174.73434781960503\n",
      "Overwriting last checkpiont with current average score: 15.35\n",
      "Episode 766\tAverage Score: 15.36\tAverage steps per second:178.52543349573054\n",
      "Overwriting last checkpiont with current average score: 15.36\n",
      "Episode 768\tAverage Score: 15.39\tAverage steps per second:185.61424554748943\n",
      "Overwriting last checkpiont with current average score: 15.39\n",
      "Episode 779\tAverage Score: 15.43\tAverage steps per second:185.61289942541412\n",
      "Overwriting last checkpiont with current average score: 15.43\n",
      "Episode 795\tAverage Score: 15.46\tAverage steps per second:193.76754758680786\n",
      "Overwriting last checkpiont with current average score: 15.46\n",
      "Episode 799\tAverage Score: 15.48\tAverage steps per second:141.16808533007705\n",
      "Overwriting last checkpiont with current average score: 15.48\n",
      "Episode 800\tAverage Score: 15.44\tAverage steps per second:161.0050788427428\n",
      "Episode 802\tAverage Score: 15.49\tAverage steps per second:131.89213529278632\n",
      "Overwriting last checkpiont with current average score: 15.49\n",
      "Episode 803\tAverage Score: 15.56\tAverage steps per second:160.9656081321654\n",
      "Overwriting last checkpiont with current average score: 15.56\n",
      "Episode 811\tAverage Score: 15.60\tAverage steps per second:163.77534057806923\n",
      "Overwriting last checkpiont with current average score: 15.6\n",
      "Episode 812\tAverage Score: 15.67\tAverage steps per second:161.4676631910884\n",
      "Overwriting last checkpiont with current average score: 15.67\n",
      "Episode 813\tAverage Score: 15.70\tAverage steps per second:163.13198790627166\n",
      "Overwriting last checkpiont with current average score: 15.7\n",
      "Episode 814\tAverage Score: 15.73\tAverage steps per second:192.0405636736022\n",
      "Overwriting last checkpiont with current average score: 15.73\n",
      "Episode 818\tAverage Score: 15.75\tAverage steps per second:179.27263760434784\n",
      "Overwriting last checkpiont with current average score: 15.75\n",
      "Episode 819\tAverage Score: 15.80\tAverage steps per second:154.50548218001973\n",
      "Overwriting last checkpiont with current average score: 15.8\n",
      "Episode 839\tAverage Score: 15.85\tAverage steps per second:176.51067828206732\n",
      "Overwriting last checkpiont with current average score: 15.85\n",
      "Episode 840\tAverage Score: 15.87\tAverage steps per second:181.5367762341267\n",
      "Overwriting last checkpiont with current average score: 15.87\n",
      "Episode 841\tAverage Score: 15.91\tAverage steps per second:175.71277916067342\n",
      "Overwriting last checkpiont with current average score: 15.91\n",
      "Episode 842\tAverage Score: 15.96\tAverage steps per second:174.71523839403685\n",
      "Overwriting last checkpiont with current average score: 15.96\n",
      "Episode 843\tAverage Score: 16.06\tAverage steps per second:170.67114931877668\n",
      "Overwriting last checkpiont with current average score: 16.06\n",
      "Episode 844\tAverage Score: 16.11\tAverage steps per second:171.8317335536921\n",
      "Overwriting last checkpiont with current average score: 16.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 845\tAverage Score: 16.17\tAverage steps per second:185.5143559118221\n",
      "Overwriting last checkpiont with current average score: 16.17\n",
      "Episode 846\tAverage Score: 16.25\tAverage steps per second:160.7005407655136\n",
      "Overwriting last checkpiont with current average score: 16.25\n",
      "Episode 900\tAverage Score: 15.62\tAverage steps per second:190.82708415311123\n",
      "Episode 1000\tAverage Score: 14.88\tAverage steps per second:192.98513175218807\n",
      "Episode 1100\tAverage Score: 16.13\tAverage steps per second:192.57585251610762\n",
      "Episode 1200\tAverage Score: 15.95\tAverage steps per second:192.39321521138992\n",
      "Episode 1213\tAverage Score: 16.28\tAverage steps per second:193.08389601495776\n",
      "Overwriting last checkpiont with current average score: 16.28\n",
      "Episode 1214\tAverage Score: 16.47\tAverage steps per second:192.23570261625758\n",
      "Overwriting last checkpiont with current average score: 16.47\n",
      "Episode 1244\tAverage Score: 16.50\tAverage steps per second:190.80638315191138\n",
      "Overwriting last checkpiont with current average score: 16.5\n",
      "Episode 1246\tAverage Score: 16.56\tAverage steps per second:192.48895169622003\n",
      "Overwriting last checkpiont with current average score: 16.56\n",
      "Episode 1248\tAverage Score: 16.58\tAverage steps per second:188.94226210896005\n",
      "Overwriting last checkpiont with current average score: 16.58\n",
      "Episode 1250\tAverage Score: 16.62\tAverage steps per second:193.27755780155297\n",
      "Overwriting last checkpiont with current average score: 16.62\n",
      "Episode 1251\tAverage Score: 16.68\tAverage steps per second:191.6100672768278\n",
      "Overwriting last checkpiont with current average score: 16.68\n",
      "Episode 1252\tAverage Score: 16.75\tAverage steps per second:189.78960799064504\n",
      "Overwriting last checkpiont with current average score: 16.75\n",
      "Episode 1254\tAverage Score: 16.82\tAverage steps per second:192.15090433416964\n",
      "Overwriting last checkpiont with current average score: 16.82\n",
      "Episode 1255\tAverage Score: 16.90\tAverage steps per second:192.42624859489058\n",
      "Overwriting last checkpiont with current average score: 16.9\n",
      "Episode 1300\tAverage Score: 16.82\tAverage steps per second:193.19249281784604\n",
      "Episode 1301\tAverage Score: 16.94\tAverage steps per second:185.35675032941143\n",
      "Overwriting last checkpiont with current average score: 16.94\n",
      "Episode 1400\tAverage Score: 16.36\tAverage steps per second:193.40727932453947\n",
      "Episode 1500\tAverage Score: 16.61\tAverage steps per second:146.50772499844334\n",
      "Episode 1600\tAverage Score: 16.67\tAverage steps per second:161.82361066647465\n",
      "Episode 1700\tAverage Score: 15.86\tAverage steps per second:167.00727783855478\n",
      "Episode 1800\tAverage Score: 15.40\tAverage steps per second:149.86308973261424\n",
      "Episode 1900\tAverage Score: 15.67\tAverage steps per second:160.61965399769232\n",
      "Episode 2000\tAverage Score: 15.77\tAverage steps per second:162.39136818012187\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_step = 0\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.99\n",
    "agent = Agent(state_size=37, \n",
    "              action_size=4,\n",
    "              t_step=t_step,\n",
    "              alpha=alpha,\n",
    "              gamma=gamma,\n",
    "              seed=0)\n",
    "\n",
    "\n",
    "n_episodes=2000\n",
    "max_t=1000\n",
    "eps_start=1.0\n",
    "eps_end=0.01\n",
    "eps_decay=0.995\n",
    "\n",
    "scores = dqn(agent, env, brain_name, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dueling DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module): # TODO\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        \n",
    "                \n",
    "        \"\"\"\n",
    "        TODO: IMPLEMENT DUELING DQNS: SEPARATE VALUE FROM ACTION\n",
    "        \n",
    "        \n",
    "        ONE NET OUTPUT WILL BE A SINGLE VALUE FOR THE STATE\n",
    "        THE OTHER OUTPUT WILL BE ONE VALUE FOR EACH ACTION ???\n",
    "        \n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.ac1 = nn.ReLU()\n",
    "        \n",
    "        # Advantage values\n",
    "        self.fc2_a = nn.Linear(64, 64)\n",
    "        self.ac2_a = nn.ReLU()\n",
    "        self.fc3_a = nn.Linear(64, action_size)\n",
    "#         self.ac3_a = nn.ReLU()\n",
    "        \n",
    "        # State value\n",
    "        self.fc2_s = nn.Linear(64, 64)\n",
    "        self.ac2_s = nn.ReLU()\n",
    "        self.fc3_s = nn.Linear(64, 1)\n",
    "#         self.ac3_s = nn.ReLU()\n",
    "\n",
    "    def preprocess_input(self, state):\n",
    "        return state\n",
    "\n",
    "    def forward(self, state): # TODO\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        # perform the forward pass\n",
    "        # x = self.preprocess_input(state)\n",
    "        x = self.ac1(self.fc1(state))\n",
    "        \n",
    "        # advantage values\n",
    "        ad = self.ac2_a(self.fc2_a(x))\n",
    "        ad = self.fc3_a(ad)\n",
    "        \n",
    "        # state value \n",
    "        sv = self.ac2_s(self.fc2_s(x))\n",
    "        sv = self.fc3_s(sv)\n",
    "        \n",
    "# #         print(f\"Advantage tensor shape: {ad.shape}. State value: {sv.shape}\")\n",
    "#         print(f\"Advantage tensor: {ad}. State value: {sv}\")\n",
    "#         print(f\"Sum: {ad+sv}\")\n",
    "        return ad + sv\n",
    "    \n",
    "    \n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 state_size, \n",
    "                 action_size,\n",
    "                 t_step=0,\n",
    "                 alpha=0.1,\n",
    "                 model_path=None,\n",
    "                 buffer_size=int(1e5),\n",
    "                 batch_size=64,\n",
    "                 gamma=0.99,\n",
    "                 tau=1e-3,\n",
    "                 learning_rate=5e-4,\n",
    "                 update_every=4,\n",
    "                 seed=0):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Q-Network\n",
    "        if not model_path:\n",
    "            print(f\"Creating new models for local and target networks.\")\n",
    "            self.qnetwork_local = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_target = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate)\n",
    "            self.training = True\n",
    "        else:\n",
    "            print(f\"Loading model from file {model_path}\")\n",
    "            self.qnetwork_local = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_target = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_local.load_state_dict(torch.load(model_path))\n",
    "            self.qnetwork_target.load_state_dict(torch.load(model_path))\n",
    "            self.training = False\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed, self.device)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = t_step\n",
    "        self.alpha = alpha\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_every = update_every\n",
    "        \n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Compute target values: # reward + gamma * max(Q(s', a')) * (1-done)\n",
    "        td_targets_next_states = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        td_targets = rewards + (gamma*td_targets_next_states * (1-dones))\n",
    "                     \n",
    "        # Current Q values\n",
    "        q_values_local = self.qnetwork_local(states).gather(1, actions) # Q\n",
    "        \n",
    "        # compute loss\n",
    "        loss = F.mse_loss(q_values_local, td_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- # # Only update the target network after we pass on the samples\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)                     \n",
    "\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "            \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "\n",
    "                \n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "            \n",
    "        if self.training:\n",
    "            self.qnetwork_local.train()\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "\n",
    "class ReplayBuffer: \n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed, device):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"../banana_unity_env/Banana.exe\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new models for local and target networks.\n",
      "Episode 100\tAverage Score: 0.99\tAverage steps per second:180.17903527472018\n",
      "Episode 200\tAverage Score: 4.37\tAverage steps per second:164.24858294154865\n",
      "Episode 300\tAverage Score: 8.25\tAverage steps per second:176.88743578452542\n",
      "Episode 400\tAverage Score: 10.29\tAverage steps per second:173.99885147431826\n",
      "Episode 500\tAverage Score: 12.71\tAverage steps per second:192.25232348261937\n",
      "Episode 600\tAverage Score: 14.43\tAverage steps per second:170.39982792849358\n",
      "Episode 700\tAverage Score: 15.21\tAverage steps per second:182.92647434421044\n",
      "Episode 800\tAverage Score: 14.95\tAverage steps per second:168.78548255526867\n",
      "Episode 900\tAverage Score: 15.58\tAverage steps per second:164.56545623297308\n",
      "Episode 1000\tAverage Score: 15.55\tAverage steps per second:165.2649089321022\n",
      "Episode 1100\tAverage Score: 15.80\tAverage steps per second:160.76353651941026\n",
      "Episode 1200\tAverage Score: 15.83\tAverage steps per second:167.42239837150917\n",
      "Episode 1300\tAverage Score: 15.75\tAverage steps per second:164.06997318435083\n",
      "Episode 1400\tAverage Score: 16.31\tAverage steps per second:165.92338504124413\n",
      "Episode 1500\tAverage Score: 15.61\tAverage steps per second:164.41053100498465\n",
      "Episode 1589\tAverage Score: 15.57\tAverage steps per second:170.07840035586224"
     ]
    }
   ],
   "source": [
    "t_step = 0\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.99\n",
    "agent = Agent(state_size=37, \n",
    "              action_size=4,\n",
    "              t_step=t_step,\n",
    "              alpha=alpha,\n",
    "              gamma=gamma,\n",
    "              seed=0)\n",
    "\n",
    "\n",
    "n_episodes=2000\n",
    "max_t=1000\n",
    "eps_start=1.0\n",
    "eps_end=0.01\n",
    "eps_decay=0.995\n",
    "\n",
    "scores = dqn(agent, env, brain_name, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting the three improvements together: Prioritized experience replay, Double DQN and Dueling DQN\n",
    "\n",
    "\n",
    "Now I implement an architecture that integrates the three improvements that were implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module): # TODO\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        \n",
    "                \n",
    "        \"\"\"\n",
    "        TODO: IMPLEMENT DUELING DQNS: SEPARATE VALUE FROM ACTION\n",
    "        \n",
    "        \n",
    "        ONE NET OUTPUT WILL BE A SINGLE VALUE FOR THE STATE\n",
    "        THE OTHER OUTPUT WILL BE ONE VALUE FOR EACH ACTION ???\n",
    "        \n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.ac1 = nn.ReLU()\n",
    "        \n",
    "        # Advantage values\n",
    "        self.fc2_a = nn.Linear(64, 64)\n",
    "        self.ac2_a = nn.ReLU()\n",
    "        self.fc3_a = nn.Linear(64, action_size)\n",
    "#         self.ac3_a = nn.ReLU()\n",
    "        \n",
    "        # State value\n",
    "        self.fc2_s = nn.Linear(64, 64)\n",
    "        self.ac2_s = nn.ReLU()\n",
    "        self.fc3_s = nn.Linear(64, 1)\n",
    "#         self.ac3_s = nn.ReLU()\n",
    "\n",
    "    def preprocess_input(self, state):\n",
    "        return state\n",
    "\n",
    "    def forward(self, state): # TODO\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        # perform the forward pass\n",
    "        # x = self.preprocess_input(state)\n",
    "        x = self.ac1(self.fc1(state))\n",
    "        \n",
    "        # advantage values\n",
    "        ad = self.ac2_a(self.fc2_a(x))\n",
    "        ad = self.fc3_a(ad)\n",
    "        \n",
    "        # state value \n",
    "        sv = self.ac2_s(self.fc2_s(x))\n",
    "        sv = self.fc3_s(sv)\n",
    "\n",
    "        return ad + sv\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 state_size, \n",
    "                 action_size,\n",
    "                 t_step=0,\n",
    "                 model_path=None,\n",
    "                 buffer_size=int(1e5),\n",
    "                 batch_size=64,\n",
    "                 gamma=0.99,\n",
    "                 tau=1e-3,\n",
    "                 learning_rate=5e-4,\n",
    "                 update_every=4,\n",
    "                 seed=0):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Q-Network\n",
    "        if not model_path:\n",
    "            self.qnetwork_local = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_target = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate)\n",
    "            self.training = True\n",
    "        else:\n",
    "            print(f\"Loading model from file {model_path}\")\n",
    "            self.qnetwork_local = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_target = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_local.load_state_dict(torch.load(model_path))\n",
    "            self.qnetwork_target.load_state_dict(torch.load(model_path))\n",
    "            self.training = False\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = PrioritizedReplayBuffer(action_size, buffer_size, batch_size, seed, self.device)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = t_step\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_every = update_every\n",
    "        \n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences      \n",
    "\n",
    "#         # Compute target values: # reward + gamma * max(Q(s', a')) * (1-done)\n",
    "#         td_targets_next_states = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "#         td_targets = rewards + (gamma*td_targets_next_states * (1-dones))\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: IMPLEMENT DOUBLE DQN: INSTEAD OF CALCULATING THE MAX VALUE USING THE TARGET NETWORK, \n",
    "        WE WILL GET THE INDEX OF THE ACTION USING THE LOCAL NETWORK. WITH THIS INDEX, WE WILL GET THE VALUE CALCULATED\n",
    "        USING THE TARGET NETWORK\n",
    "        \n",
    "        From the paper:\n",
    "        \"In the original Double Q-learning algorithm, two value\n",
    "        functions are learned by assigning each experience randomly to update one of the two value functions, such that\n",
    "        there are two sets of weights, θ and θ`. For each update, one\n",
    "        set of weights is used to determine the greedy policy and the\n",
    "        other to determine its value. For a clear comparison, we can\n",
    "        first untangle the selection and evaluation in Q-learning...\"\n",
    "        \n",
    "        PSEUDOCODE\n",
    "        td_targets_next_states_indexes = np.argmax(self.qnetwork_local(next_states), axis=1)\n",
    "        td_targets_next_states = self.qnetwork_target(next_states).gather(td_targets_next_states_indexes)\n",
    "        \n",
    "        \"\"\"\n",
    "        td_targets_next_states_indexes = np.argmax(self.qnetwork_local(next_states).detach(), axis=1).unsqueeze(1) # Get the action index using the local network\n",
    "        td_targets_next_states = self.qnetwork_target(next_states).detach().gather(1, td_targets_next_states_indexes) # use the indexes to get the values from the target network\n",
    "        td_targets = rewards + (gamma*td_targets_next_states * (1-dones)) # calculate the target just as before\n",
    "        \n",
    "        \n",
    "        # Current Q values\n",
    "        q_values_local = self.qnetwork_local(states).gather(1, actions) # Q\n",
    "        \n",
    "        # compute loss\n",
    "        loss = F.mse_loss(q_values_local, td_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- # # Only update the target network after we pass on the samples\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)                     \n",
    "\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "            \n",
    "    def calculate_error_delta(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        TODO: compute the error delta for the current state, action, reward and next_stat set\n",
    "        \"\"\"\n",
    "        # Compute target values: # reward + gamma * max(Q(s', a')) * (1-done)\n",
    "        # transform input state to torch structure\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        next_state = torch.from_numpy(next_state).float().unsqueeze(0).to(self.device)\n",
    "        action = np.asarray(action)\n",
    "        action = torch.from_numpy(action).long().unsqueeze(0).to(self.device)\n",
    "        action = action.expand(1,1)\n",
    "#         print(f\"State: {state.size()}. Next state: {next_state.size()}. Action: {action.size()}\")\n",
    "        \n",
    "        # VANILA DQN. DO DOUBLE DQN LATER\n",
    "#         td_targets_next_state = self.qnetwork_target(next_state).detach().max(1)[0].unsqueeze(1)\n",
    "#         td_target = reward + (gamma*td_targets_next_state * (1-done)) # Target\n",
    "        \n",
    "        # Using double DQN\n",
    "        td_targets_next_states_indexes = np.argmax(self.qnetwork_local(next_state).detach(), axis=1).unsqueeze(1) # Get the action index using the local network\n",
    "        td_targets_next_states = self.qnetwork_target(next_state).detach().gather(1, td_targets_next_states_indexes) # use the indexes to get the values from the target network\n",
    "        td_target = reward + (gamma*td_targets_next_states * (1-done)) # calculate the target just as before\n",
    "        \n",
    "        q_value_local = self.qnetwork_local(state).gather(1, action) # Q current\n",
    "        return abs(td_target - q_value_local)\n",
    "\n",
    "            \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: add error delta to the memory along each entry\n",
    "        \"\"\"\n",
    "        error = self.calculate_error_delta(state, action, reward, next_state, done)\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done, error)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "\n",
    "                \n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "            \n",
    "        if self.training:\n",
    "            self.qnetwork_local.train()\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed, device):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\", \"error\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.a = 1\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done, error):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done, error)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        \n",
    "        # Choose samples based on the probability, which is proportional to the error \n",
    "        priorities = np.asarray([e.error for e in self.memory], dtype=np.float64)\n",
    "        with torch.no_grad():\n",
    "            probabilities = priorities / sum(priorities) # It gives out a torch grad error, but this has never been in the graph, so why????? \n",
    "        experiences = random.choices(self.memory, weights=probabilities)\n",
    "        \n",
    "#         # now it is choosing the samples randomly. Change it to use the prioritized formula\n",
    "#         experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"../banana_unity_env/Banana.exe\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.72\tAverage steps per second:77.03412870351812\n",
      "Episode 200\tAverage Score: 2.31\tAverage steps per second:50.677854696560964\n",
      "Episode 295\tAverage Score: 5.16\tAverage steps per second:30.312363527859297"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-d4fc6cc9beee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0meps_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.995\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_end\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.995\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# plot the scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-baa50efbbfd8>\u001b[0m in \u001b[0;36mdqn\u001b[1;34m(agent, env, brain_name, model_name, n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-9ea395a74efd>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[1;31m# If enough samples are available in memory, get random subset and learn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m                 \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-9ea395a74efd>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m         \u001b[0mnext_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m         \u001b[0mdones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads\\miniconda\\envs\\drlnd\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t_step = 0\n",
    "gamma = 0.99\n",
    "epsilon = 0.99\n",
    "agent = Agent(state_size=37, \n",
    "              action_size=4,\n",
    "              t_step=t_step,\n",
    "              gamma=gamma,\n",
    "              seed=0)\n",
    "\n",
    "\n",
    "n_episodes=2000\n",
    "max_t=1000\n",
    "eps_start=1.0\n",
    "eps_end=0.01\n",
    "eps_decay=0.995\n",
    "\n",
    "scores = dqn(agent, env, brain_name, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After checking with ChatGPT\n",
    "\n",
    "Since we are in 2023, it would be imprudent of me to not use a LLM for double checking my result. I explained what I was doing and what the code does, asked if it was correct and pasted it. The result from OpenAI's ChatGPT was:\n",
    "\n",
    "\"The code seems to be mostly correct, but there are some parts that could use some improvement. Here are some suggestions:\n",
    "\n",
    "1. Dueling DQN: The code includes the basic structure of a Dueling DQN network, but it is missing the final step of combining the advantage and state-value streams. You should add ad - torch.mean(ad) after ad = self.fc3_a(ad) and sv.expand(x.size(0), self.action_size) after sv = self.fc3_s(sv).\n",
    "\n",
    "2. Double DQN: The code includes the basic structure of a Double DQN agent, but it is missing the final step of selecting the action based on the local network and getting the value based on the target network. You should add local_actions = self.qnetwork_local(next_states).detach().max(1)[1].unsqueeze(1) after local_values = self.qnetwork_local(next_states).gather(1, actions), and then replace td_targets_next_states = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1) with td_targets_next_states = self.qnetwork_target(next_states).gather(1, local_actions).\n",
    "\n",
    "3. Prioritized Experience Replay: The code includes the basic structure of a PER buffer, but it is missing the implementation of the priorities. You should add max_prio = self.memory.max_priority() if self.memory.tree else 1.0 after states, actions, rewards, next_states, dones, indices, weights = self.memory.sample() to get the maximum priority in the tree, and then replace self.memory.add(states, actions, rewards, next_states, dones) with self.memory.add(states, actions, rewards, next_states, dones, max_prio) to include the priority in the buffer.\n",
    "\n",
    "4. Training vs. evaluation: The code includes a flag self.training to indicate whether the agent is in training mode or evaluation mode. However, this flag is not used consistently throughout the code. You should update the code to use this flag consistently, for example by adding an if self.training: block around the loss.backward() and self.optimizer.step() lines in the learn method.\n",
    "\n",
    "5. Miscellaneous: There are a few minor issues with the code that don't affect the correctness of the algorithm, but could be improved for readability and performance. For example, you can use F.relu instead of defining a ReLU layer separately, and you can use F.mse_loss instead of defining a mean squared error loss function separately. You can also remove the preprocess_input method from the QNetwork class, since it is not used.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING CHATGPT CORRECTIONS ON DOUBLE DQN AND DUELING DQN. REMOVED PRIORITIZED EXPERIENCE REPLAY FOR NOW BECAUSE IT HINDERS THE TRAINING\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "\n",
    "        # Advantage values\n",
    "        self.fc2_a = nn.Linear(64, 64)\n",
    "        self.fc3_a = nn.Linear(64, action_size)\n",
    "\n",
    "        # State value\n",
    "        self.fc2_s = nn.Linear(64, 64)\n",
    "        self.fc3_s = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        advantage = F.relu(self.fc2_a(x))\n",
    "        advantage = self.fc3_a(advantage)\n",
    "        state_value = F.relu(self.fc2_s(x))\n",
    "        state_value = self.fc3_s(state_value)\n",
    "        q_values = state_value + advantage - advantage.mean()\n",
    "        return q_values\n",
    "    \n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 state_size, \n",
    "                 action_size,\n",
    "                 t_step=0,\n",
    "                 model_path=None,\n",
    "                 buffer_size=int(1e5),\n",
    "                 batch_size=64,\n",
    "                 gamma=0.99,\n",
    "                 tau=1e-3,\n",
    "                 learning_rate=5e-4,\n",
    "                 update_every=4,\n",
    "                 seed=0):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Q-Network\n",
    "        if not model_path:\n",
    "            self.qnetwork_local = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_target = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate)\n",
    "            self.training = True\n",
    "        else:\n",
    "            print(f\"Loading model from file {model_path}\")\n",
    "            self.qnetwork_local = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_target = QNetwork(state_size, action_size, seed).to(self.device)\n",
    "            self.qnetwork_local.load_state_dict(torch.load(model_path))\n",
    "            self.qnetwork_target.load_state_dict(torch.load(model_path))\n",
    "            self.training = False\n",
    "        \n",
    "        # Replay memory\n",
    "#         self.memory = PrioritizedReplayBuffer(action_size, buffer_size, batch_size, seed, self.device)\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed, self.device)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = t_step\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_every = update_every\n",
    "        \n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences      \n",
    "\n",
    "#         # Compute target values: # reward + gamma * max(Q(s', a')) * (1-done)\n",
    "#         td_targets_next_states = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "#         td_targets = rewards + (gamma*td_targets_next_states * (1-dones))\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: IMPLEMENT DOUBLE DQN: INSTEAD OF CALCULATING THE MAX VALUE USING THE TARGET NETWORK, \n",
    "        WE WILL GET THE INDEX OF THE ACTION USING THE LOCAL NETWORK. WITH THIS INDEX, WE WILL GET THE VALUE CALCULATED\n",
    "        USING THE TARGET NETWORK\n",
    "        \n",
    "        From the paper:\n",
    "        \"In the original Double Q-learning algorithm, two value\n",
    "        functions are learned by assigning each experience randomly to update one of the two value functions, such that\n",
    "        there are two sets of weights, θ and θ`. For each update, one\n",
    "        set of weights is used to determine the greedy policy and the\n",
    "        other to determine its value. For a clear comparison, we can\n",
    "        first untangle the selection and evaluation in Q-learning...\"\n",
    "        \n",
    "        PSEUDOCODE\n",
    "        td_targets_next_states_indexes = np.argmax(self.qnetwork_local(next_states), axis=1)\n",
    "        td_targets_next_states = self.qnetwork_target(next_states).gather(td_targets_next_states_indexes)\n",
    "        \n",
    "        \"\"\"\n",
    "#         td_targets_next_states_indexes = np.argmax(self.qnetwork_local(next_states).detach(), axis=1).unsqueeze(1) # Get the action index using the local network\n",
    "#         td_targets_next_states = self.qnetwork_target(next_states).detach().gather(1, td_targets_next_states_indexes) # use the indexes to get the values from the target network\n",
    "#         td_targets = rewards + (gamma*td_targets_next_states * (1-dones)) # calculate the target just as before\n",
    "    \n",
    "        # Get index of max Q values of next states from local model\n",
    "        next_state_actions = self.qnetwork_local(next_states).detach().max(1)[1].unsqueeze(1) # DOES THIS RETURN THE INDEX OF THE MAX VALUE????\n",
    "\n",
    "        # Get max Q values of next states from target model\n",
    "        td_targets_next_states = self.qnetwork_target(next_states).gather(1, next_state_actions)\n",
    "\n",
    "        # Compute Q targets for current states \n",
    "        td_targets = rewards + (gamma * td_targets_next_states * (1 - dones))    \n",
    "    \n",
    "        \n",
    "        # Current Q values\n",
    "        q_values_local = self.qnetwork_local(states).gather(1, actions) # Q\n",
    "        \n",
    "        # compute loss\n",
    "        loss = F.mse_loss(q_values_local, td_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- # # Only update the target network after we pass on the samples\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)                     \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "            \n",
    "    def calculate_error_delta(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        TODO: compute the error delta for the current state, action, reward and next_stat set\n",
    "        \"\"\"\n",
    "        # Compute target values: # reward + gamma * max(Q(s', a')) * (1-done)\n",
    "        # transform input state to torch structure\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        next_state = torch.from_numpy(next_state).float().unsqueeze(0).to(self.device)\n",
    "        action = np.asarray(action)\n",
    "        action = torch.from_numpy(action).long().unsqueeze(0).to(self.device)\n",
    "        action = action.expand(1,1)\n",
    "#         print(f\"State: {state.size()}. Next state: {next_state.size()}. Action: {action.size()}\")\n",
    "        \n",
    "        # VANILA DQN. DO DOUBLE DQN LATER\n",
    "#         td_targets_next_state = self.qnetwork_target(next_state).detach().max(1)[0].unsqueeze(1)\n",
    "#         td_target = reward + (gamma*td_targets_next_state * (1-done)) # Target\n",
    "        \n",
    "        # Using double DQN\n",
    "        td_targets_next_states_indexes = np.argmax(self.qnetwork_local(next_state).detach(), axis=1).unsqueeze(1) # Get the action index using the local network\n",
    "        td_targets_next_states = self.qnetwork_target(next_state).detach().gather(1, td_targets_next_states_indexes) # use the indexes to get the values from the target network\n",
    "        td_target = reward + (gamma*td_targets_next_states * (1-done)) # calculate the target just as before\n",
    "        \n",
    "        # CHATGPT VERSION\n",
    "#         # Get index of max Q values of next states from local model\n",
    "#         next_state_actions = self.qnetwork_local(next_states).detach().max(1)[1].unsqueeze(1) # DOES THIS RETURN THE INDEX OF THE MAX VALUE????\n",
    "\n",
    "#         # Get max Q values of next states from target model\n",
    "#         td_targets_next_states = self.qnetwork_target(next_states).gather(1, next_state_actions)\n",
    "\n",
    "#         # Compute Q targets for current states \n",
    "#         td_targets = rewards + (gamma * td_targets_next_states * (1 - dones))  \n",
    "        \n",
    "        q_value_local = self.qnetwork_local(state).gather(1, action) # Q current\n",
    "        return abs(td_target - q_value_local)\n",
    "\n",
    "            \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: add error delta to the memory along each entry\n",
    "        \"\"\"\n",
    "#         error = self.calculate_error_delta(state, action, reward, next_state, done)\n",
    "#         # Save experience in replay memory\n",
    "#         self.memory.add(state, action, reward, next_state, done, error)\n",
    "\n",
    "        \n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)        \n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "\n",
    "                \n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "            \n",
    "        if self.training:\n",
    "            self.qnetwork_local.train()\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed, device):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\", \"error\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.a = 1\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done, error):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done, error)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        \n",
    "        # Choose samples based on the probability, which is proportional to the error \n",
    "        priorities = np.asarray([e.error for e in self.memory], dtype=np.float64)\n",
    "        with torch.no_grad():\n",
    "            probabilities = priorities / sum(priorities) # It gives out a torch grad error, but this has never been in the graph, so why????? \n",
    "        experiences = random.choices(self.memory, weights=probabilities)\n",
    "        \n",
    "#         # now it is choosing the samples randomly. Change it to use the prioritized formula\n",
    "#         experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "  \n",
    "    \n",
    "class ReplayBuffer: \n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed, device):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"../banana_unity_env/Banana.exe\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 1.49\tAverage steps per second:159.65398449133576\n",
      "Episode 200\tAverage Score: 5.42\tAverage steps per second:158.46926173940315\n",
      "Episode 300\tAverage Score: 8.38\tAverage steps per second:163.56844014718456\n",
      "Episode 400\tAverage Score: 10.97\tAverage steps per second:161.41942562292917\n",
      "Episode 500\tAverage Score: 13.32\tAverage steps per second:160.19100036276498\n",
      "Episode 600\tAverage Score: 14.53\tAverage steps per second:172.87801682604496\n",
      "Episode 700\tAverage Score: 15.00\tAverage steps per second:176.51636760198114\n",
      "Episode 800\tAverage Score: 14.42\tAverage steps per second:173.71031043401644\n",
      "Episode 900\tAverage Score: 15.35\tAverage steps per second:174.26820249845723\n",
      "Episode 1000\tAverage Score: 15.73\tAverage steps per second:175.4291802075225\n",
      "Episode 1100\tAverage Score: 16.27\tAverage steps per second:177.84715997619244\n",
      "Episode 1200\tAverage Score: 15.64\tAverage steps per second:182.65775276372307\n",
      "Episode 1300\tAverage Score: 15.86\tAverage steps per second:179.90678220552402\n",
      "Episode 1400\tAverage Score: 15.49\tAverage steps per second:180.62031746680998\n",
      "Episode 1500\tAverage Score: 14.75\tAverage steps per second:181.93732554806198\n",
      "Episode 1600\tAverage Score: 15.11\tAverage steps per second:179.85721759722438\n",
      "Episode 1700\tAverage Score: 15.36\tAverage steps per second:171.68137358580867\n",
      "Episode 1800\tAverage Score: 16.36\tAverage steps per second:176.81030044854745\n",
      "Episode 1900\tAverage Score: 16.60\tAverage steps per second:122.66802249678999\n",
      "Episode 2000\tAverage Score: 16.04\tAverage steps per second:151.24388435198992\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_step = 0\n",
    "gamma = 0.99\n",
    "epsilon = 0.99\n",
    "agent = Agent(state_size=37, \n",
    "              action_size=4,\n",
    "              t_step=t_step,\n",
    "              gamma=gamma,\n",
    "              seed=0)\n",
    "\n",
    "\n",
    "scores = dqn(agent, \n",
    "             env, \n",
    "             brain_name, \n",
    "             n_episodes=2000, \n",
    "             max_t=1000, \n",
    "             eps_start=1.0, \n",
    "             eps_end=0.01, \n",
    "             eps_decay=0.995)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "As shown in this project, the vanilla DQN architecture has shown to outperform the architectures with improvements, yet all of them were able to solve the environment. I am not sure why this is the case, though I suspect the fault lies either on the implementation, or on the setting of hyperparameters. \n",
    "\n",
    "The prioritized experience replay was the worse upgrade, because it had a huge impact on the training performance. In the next session we will discuss improvements for it.\n",
    "\n",
    "Since time is of essence, and all algorithms have solved the environment, I am not spending more time investigating and tyring to improve it right now. Once the course is over, I will come back to this project, read the papers more thoroughly and fine tune the hyperparameters to improve the result.\n",
    "\n",
    "Also, I intend to implement the remaining three improvements to complete the Rainbow architecture for the DQN. I shall then apply the algorithm to many different Gym and Unity ML-Agents environments, creating a single repository with hyperparameter configuration filies for each env.\n",
    "\n",
    "\n",
    "## Note on Hyperparameters\n",
    "\n",
    "In this simple example of the DQN algorithm, there are several model and training hyperparameters:\n",
    "\n",
    "#### Model:\n",
    "\n",
    "- Number and types of layers\n",
    "- Layers width\n",
    "- Activation layer\n",
    "- Dropout layer\n",
    "\n",
    "#### Agent\n",
    "\n",
    "\n",
    "- buffer_size: Size of the memory\n",
    "- batch_size: number of memory entries that will be trained at each learn step simultaneously\n",
    "- gamma: discount rate for multiplying the next state action value when calculating the target\n",
    "- tau: soft update parameter used when updating the target network with the local network weights\n",
    "- learning_rate: weights delta multiplier for scaling the update size\n",
    "- t_step: used to calculate if it's time to updade\n",
    "- update_every: number of steps before updating (learning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
